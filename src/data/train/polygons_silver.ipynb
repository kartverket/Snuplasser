{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5de88eb-92c2-4c06-9c99-ef5918ad0474",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f0bc67b-e8a2-4174-9444-1e794cbc7b05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max\n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructType, StructField, IntegerType, LongType, FloatType, TimestampType, DateType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from sedona.spark import *\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import re\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad7ca19e-e775-4f9c-99b7-1359c7c4bad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "log_table = \"logs_processed_gdbs\"\n",
    "bronze_table = \"polygons_bronze\"\n",
    "silver_table = \"polygons_silver\"\n",
    "buffer = 20  # buffer rundt polygonet gir un√∏yaktig st√∏rrelse hvis f.eks. pikselverdi er tenk √• v√¶re 0.25kvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d428224e-0b4c-4101-a816-d43a4cd55485",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_bronze = spark.read.table(bronze_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea788db4-1dc3-49b8-a629-ed094a77048e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BRUKERID = os.getenv(\"GEONORGE_BRUKERID\")\n",
    "PASSORD  = os.getenv(\"GEONORGE_PASSORD\")\n",
    "\n",
    "def get_token():\n",
    "    url = (\n",
    "        f\"https://baat.geonorge.no/skbaatts/req?brukerid={BRUKERID}\"\n",
    "        f\"&passord={PASSORD}&tjenesteid=wms.nib&retformat=s\"\n",
    "    )\n",
    "    raw_token = requests.get(url).text.strip(\"`\")\n",
    "    return raw_token\n",
    "\n",
    "token = get_token()\n",
    "token_start_time = time.time()\n",
    "token_lifetime = 55 * 60  # sekunder\n",
    "\n",
    "def refresh_token_if_needed():\n",
    "    global token, token_start_time\n",
    "    if time.time() - token_start_time > token_lifetime:\n",
    "        print(\"üîÑ Fornyer token...\")\n",
    "        token = get_token()\n",
    "        token_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af76e18a-046e-46ef-88ed-8f587fb6c6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_table_to_wkt() -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read a Spark DataFrame column containing WKT and return a Spark DataFrame with the\n",
    "    \"\"\"\n",
    "    df_bronze = spark.read.table(bronze_table).withColumn(\"geometry\", F.expr(\"ST_GeomFromWKT(geometry)\"))\n",
    "    return df_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c77d6a-d1ee-4d7a-97ec-ec498b688015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_envelope(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Make a envelope based on the geom.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"envelope\", F.expr(\"ST_Boundary(geometry)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fb0baad-55f9-4284-913c-6b0bd1b0c015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def random_adjusted_bbox_centered(\n",
    "    envelope: list,\n",
    "    min_size: int = 256,\n",
    "    max_size: int = 256,\n",
    "    margin: int = 30,\n",
    "    max_offset: float = 80,  # margin - f√• meter\n",
    "    max_attempts: int = 10\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Lager en justert BBOX som garanterer at polygonet (envelope) er synlig og ligger innenfor BBOX,\n",
    "    med en viss variasjon i st√∏rrelse og plassering.\n",
    "\n",
    "    Args:\n",
    "        envelope: [xmin, ymin, xmax, ymax]\n",
    "        min_size: Minimum st√∏rrelse p√• BBOX (f.eks. 256 m)\n",
    "        max_size: Maksimal st√∏rrelse (f.eks. 512 m)\n",
    "        margin: Buffer rundt polygonet (f.eks. 40 m)\n",
    "        max_offset: Maks tilfeldig forskyvning fra sentrum (f.eks. ¬±40 m)\n",
    "        max_attempts: Antall fors√∏k f√∏r feilmelding\n",
    "\n",
    "    Returns:\n",
    "        (bbox: list, bbox_str: str) ‚Äì bbox = [xmin, ymin, xmax, ymax]\n",
    "    \"\"\"\n",
    "    import random\n",
    "\n",
    "    xmin, ymin, xmax, ymax = envelope\n",
    "    poly_width = xmax - xmin\n",
    "    poly_height = ymax - ymin\n",
    "\n",
    "    # Beregn √∏nsket BBOX-st√∏rrelse\n",
    "    bbox_size = max(poly_width, poly_height) + margin * 2\n",
    "    bbox_size = min(max(bbox_size, min_size), max_size)\n",
    "    half_size = bbox_size / 2\n",
    "\n",
    "    # Polygonets sentrum\n",
    "    center_x_orig = (xmin + xmax) / 2\n",
    "    center_y_orig = (ymin + ymax) / 2\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        dx = random.uniform(-max_offset, max_offset)\n",
    "        dy = random.uniform(-max_offset, max_offset)\n",
    "\n",
    "        center_x = center_x_orig + dx\n",
    "        center_y = center_y_orig + dy\n",
    "\n",
    "        # Lag BBOX\n",
    "        adjusted_xmin = center_x - half_size\n",
    "        adjusted_xmax = center_x + half_size\n",
    "        adjusted_ymin = center_y - half_size\n",
    "        adjusted_ymax = center_y + half_size\n",
    "\n",
    "        # Sjekk at hele polygonet er innenfor den justerte BBOX-en\n",
    "        if (adjusted_xmin <= xmin and adjusted_ymin <= ymin and\n",
    "            adjusted_xmax >= xmax and adjusted_ymax >= ymax):\n",
    "            bbox = [adjusted_xmin, adjusted_ymin, adjusted_xmax, adjusted_ymax]\n",
    "            bbox_str = \"_\".join(f\"{v:.6f}\" for v in bbox)\n",
    "            return bbox, bbox_str\n",
    "\n",
    "    raise ValueError(\"Fant ikke gyldig adjusted_bbox etter flere fors√∏k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6dc4af8-c5bf-49d6-a291-e8acc653f94d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_bbox(df: DataFrame, column_name: str) -> DataFrame: \n",
    "    \"\"\"\n",
    "    Make a bounding box from a Spark DataFrame based on the envelope.\n",
    "    \"\"\"\n",
    "    df = df.withColumn(\n",
    "    \"bbox\",\n",
    "    F.expr(f\"\"\"\n",
    "    array(\n",
    "        ST_X(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_Y(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_X(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "        ST_Y(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer})\n",
    "    )\n",
    "    \"\"\") # Lager BBOX kolonne som kan brukes videre i WMSene p√• en enkel m√•te.\n",
    "    )\n",
    "    df = df.withColumn(\n",
    "        \"Polygons\",\n",
    "        F.expr(\"ST_MakeEnvelope(bbox[0], bbox[1], bbox[2], bbox[3])\")\n",
    "    ) # Lager polygons basert p√• bboxen \n",
    "    df = df.withColumn(\n",
    "        \"adjusted_struct\",\n",
    "        adjusted_bbox_udf(F.col(\"bbox\"))\n",
    "    ).withColumn(\n",
    "        \"Adjusted_bbox\", F.col(\"adjusted_struct.bbox\")\n",
    "    ).withColumn(\n",
    "        \"bbox_str\", F.col(\"adjusted_struct.bbox_str\")\n",
    "    ).drop(\"envelope\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "520ba67d-c891-4773-9518-6194c88f01a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dom_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.hoyde-dom-nhm-25833?request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=NHM_DOM_25833:skyggerelieff&\"\n",
    "        f\"BBOX={bbox_str}&width={width}&height={height}\"\n",
    "    )\n",
    "\n",
    "def generate_image_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.nib?VERSION=1.3.0\"\n",
    "        f\"&service=WMS&request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=ortofoto&\"\n",
    "        f\"BBox={bbox_str}&width={width}&height={height}&TICKET=\"\n",
    "    )\n",
    "\n",
    "def dom_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/doms/dom_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def image_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/images/image_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def mask_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/labels/mask_{id}.png\"\n",
    "    return \"GENERATED\" if os.path.exists(path) else \"PENDING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3c5585e-df5d-47c6-8957-497fdc5a5131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_fotodato(bbox: str, token: str):\n",
    "    url = f\"https://wms.geonorge.no/skwms1/wms.nib?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&CRS=EPSG:25833&BBOX={bbox}&WIDTH=512&HEIGHT=512&LAYERS=ortofoto&QUERY_LAYERS=ortofoto&INFO_FORMAT=text/html&I=256&J=256&TICKET={token}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\")\n",
    "    field_value = None\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) >= 2 and cells[0].text.strip() == \"Fotodato\":\n",
    "                field_value = cells[1].text.strip()\n",
    "                field_value = datetime.strptime(field_value, \"%d.%m.%Y\").date()\n",
    "                return field_value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43f6cb7d-892c-44f9-8785-28bd1ab44e42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ortofoto_date(df, token: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Add the ortofoto date to the dataframe based on bbox.\n",
    "    Returns a new DataFrame with the 'fototid' column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only relevant columns for processing\n",
    "    sample_rows = df.select(\"row_hash\", \"bbox_str\").collect()\n",
    "\n",
    "    # Fetch fotodato values from WMS\n",
    "    bbox_date_pairs = [\n",
    "        (row[\"row_hash\"], get_fotodato(row[\"bbox_str\"].replace('_', ','), token))\n",
    "        for row in sample_rows\n",
    "    ]\n",
    "\n",
    "    # Schema for intermediate DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"row_hash\", StringType(), True),\n",
    "        StructField(\"fototid\", DateType(), True)  # Use StringType if needed\n",
    "    ])\n",
    "\n",
    "    # Create small lookup DataFrame\n",
    "    bbox_date_df = spark.createDataFrame(bbox_date_pairs, schema)\n",
    "\n",
    "    # Join back on row_hash\n",
    "    df_with_date = df.join(bbox_date_df, on=\"row_hash\", how=\"left\")\n",
    "\n",
    "    return df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a127e380-443c-4960-bd52-b19484f5bb87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write Spark DataFrame to Delta table.\n",
    "    Automatically updates or inserts all columns.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(\"source\"),\n",
    "                condition=\"target.row_hash = source.row_hash\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca2520ed-20d7-48f6-80ea-7d2f4807c2ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot(df: DataFrame, column_name: str, plot_column: str):\n",
    "    \"\"\"\n",
    "    Plot the bbox.\n",
    "    \"\"\"\n",
    "    bbox_gdf = gpd.GeoDataFrame(\n",
    "    df.toPandas(),\n",
    "    geometry=column_name,\n",
    "    crs=\"EPSG:25833\",  # Bruker ETRS89 / UTM sone 33N (Norge)\n",
    "    )\n",
    "    return bbox_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c927767-2831-4a12-bd25-1a8867ce8d9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adjusted_bbox_schema = StructType([\n",
    "    StructField(\"bbox\", ArrayType(DoubleType())),\n",
    "    StructField(\"bbox_str\", StringType())\n",
    "])\n",
    "adjusted_bbox_udf = F.udf(lambda envelope: random_adjusted_bbox_centered(envelope), adjusted_bbox_schema)\n",
    "generate_dom_url_udf = F.udf(generate_dom_url, StringType())\n",
    "generate_image_url_udf = F.udf(generate_image_url, StringType())\n",
    "dom_file_status_udf = F.udf(dom_file_exists, StringType())\n",
    "image_file_status_udf = F.udf(image_file_exists, StringType())\n",
    "mask_file_exists_udf = F.udf(mask_file_exists, StringType())\n",
    "\n",
    "BASE_PATH = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER\"\n",
    "\n",
    "SUBDIR = {\n",
    "    \"image\": \"images\",\n",
    "    \"dom\":   \"doms\",\n",
    "    \"mask\":  \"labels\"   \n",
    "}\n",
    "\n",
    "BASE_PATH = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER\"\n",
    " \n",
    "SUBDIR = {\n",
    "    \"image\": \"images\",\n",
    "    \"dom\":   \"doms\",\n",
    "    \"mask\":  \"labels\"  \n",
    "}\n",
    " \n",
    "df = read_table_to_wkt()\n",
    "df = make_envelope(df)\n",
    "df = make_bbox(df, \"envelope\")\n",
    " \n",
    "for dt in [\"image\", \"dom\", \"mask\"]:\n",
    "    sub = SUBDIR[dt]\n",
    "    df = df.withColumn(\n",
    "        f\"{dt}_path\",\n",
    "        F.concat(\n",
    "            F.lit(f\"{BASE_PATH}/{sub}/{dt}_\"),\n",
    "            F.col(\"row_hash\"),\n",
    "            F.lit(\".png\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "df = df.withColumn(\"image_wms\", generate_image_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"dom_wms\", generate_dom_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"image_status\", image_file_status_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"dom_status\", dom_file_status_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"mask_status\", mask_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"lastet_tid\", F.current_timestamp()) \n",
    "\n",
    "df = add_ortofoto_date(df, token)\n",
    "\n",
    "\n",
    "#gdf = plot(df, \"Polygons\", \"Vegtyper\")\n",
    "write_delta_table(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381991c3-866f-4ced-b72e-e5ff04ea3cd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_retable = spark.read.table(silver_table)\n",
    "df_overview = df_retable.drop(\n",
    "    \"Shape_Length\",\n",
    "    \"Shape_Area\",\n",
    "    \"Vegtyper\",\n",
    "    \"Snuplasstype\",\n",
    "    \"Parkering\",\n",
    "    \"geometry\",\n",
    "    \"ingest_time\",\n",
    "    \"source_file\",\n",
    "    \"source_layer\",\n",
    "    \"bbox\",\n",
    "    \"Polygons\",\n",
    "    \"adjusted_struct\",\n",
    "    \"Adjusted_bbox\",\n",
    "    \"bbox_str\",\n",
    "    \"image_wms\",\n",
    "    \"dom_wms\",\n",
    "    \"lastet_tid\",\n",
    "    \"fototid\"\n",
    ")\n",
    "\n",
    "df_overview.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"polygons_status_overview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647f007d-0308-446c-bebe-a4b1881e401e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c113dd4a-77ed-4196-8a1a-edb6d028a7b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# table = \"polygons_status_overview\"\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5724bec4-25f2-4cdc-aa9e-6991bc8f2901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment under hvis man √∏nsker √• se p√• plotten\n",
    "# gdf.explore(column=\"Vegtyper\", tooltip=\"Vegtyper\", popup=True, cmap=\"Set1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c17aeb0-6d4b-4573-9727-11e96fb70da8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5031113679203828,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "polygons_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
