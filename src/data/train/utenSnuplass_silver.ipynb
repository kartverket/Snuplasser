{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b74a3-f832-42dd-ac71-fdfc4ed71bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv\n",
    "%pip install apache-sedona\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce3e967-c0e0-4bf1-82ed-911d7b28d1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, when \n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructType, StructField, IntegerType, LongType, FloatType, DateType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from sedona.spark import *\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.errors import WKTReadingError\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e43ca0-cc6e-43d2-877e-f41d8f77001b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "bronze_table = \"utensnuplass_bronze\"\n",
    "silver_table = \"utensnuplass_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4786fe-f2cb-4f33-ba78-ceabd1d61c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_geometry_from_column()-> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the geometries from the bronze table and returns a dataframe with the geometries as a column.\n",
    "    \"\"\"\n",
    "    df_bronze= spark.read.table(bronze_table).withColumn(\"geometry\", F.expr(\"ST_GeomFromWKT(bbox)\"))\n",
    "    return df_bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d557b5-79d4-490d-804b-62101360a7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_envolope_column(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column with the envelope of the geometries.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"envelope\", F.expr(\"ST_Boundary(geometry)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d3a263-fcfa-4109-adcc-ca08342531ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def random_adjusted_bbox_centered(\n",
    "    envelope: list,\n",
    "    min_size: int = 256,\n",
    "    max_size: int = 256,\n",
    "    margin: int = 30,\n",
    "    max_offset: float = 80,  # margin - få meter\n",
    "    max_attempts: int = 10\n",
    ") -> list:\n",
    "    \n",
    "    import random\n",
    "\n",
    "    xmin, ymin, xmax, ymax = envelope\n",
    "    poly_width = xmax - xmin\n",
    "    poly_height = ymax - ymin\n",
    "\n",
    "    # Beregn ønsket BBOX-størrelse\n",
    "    bbox_size = max(poly_width, poly_height) + margin * 2\n",
    "    bbox_size = min(max(bbox_size, min_size), max_size)\n",
    "    half_size = bbox_size / 2\n",
    "\n",
    "    # Polygonets sentrum\n",
    "    center_x_orig = (xmin + xmax) / 2\n",
    "    center_y_orig = (ymin + ymax) / 2\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        dx = random.uniform(-max_offset, max_offset)\n",
    "        dy = random.uniform(-max_offset, max_offset)\n",
    "\n",
    "        center_x = center_x_orig + dx\n",
    "        center_y = center_y_orig + dy\n",
    "\n",
    "        # Lag BBOX\n",
    "        adjusted_xmin = center_x - half_size\n",
    "        adjusted_xmax = center_x + half_size\n",
    "        adjusted_ymin = center_y - half_size\n",
    "        adjusted_ymax = center_y + half_size\n",
    "\n",
    "        # Sjekk at hele polygonet er innenfor den justerte BBOX-en\n",
    "        if (adjusted_xmin <= xmin and adjusted_ymin <= ymin and\n",
    "            adjusted_xmax >= xmax and adjusted_ymax >= ymax):\n",
    "            bbox = [adjusted_xmin, adjusted_ymin, adjusted_xmax, adjusted_ymax]\n",
    "            bbox_str = \"_\".join(f\"{v:.6f}\" for v in bbox)\n",
    "            return {\"bbox\" : bbox, \"bbox_str\" : bbox_str}\n",
    "\n",
    "    raise ValueError(\"Fant ikke gyldig adjusted_bbox etter flere forsøk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5efd03-3fce-4eb4-8440-974cf5707245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_bbox(df: DataFrame, buffer: float = 20.0) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a bounding box for each row based on the 'envelope' geometry,\n",
    "    expands it slightly with a buffer, and returns random adjusted boxes.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame with a 'geometry' column (as ST_GeomFromWKT).\n",
    "        buffer: Extra margin (in meters) added around the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with added columns: bbox, bbox_str, Adjusted_bbox, Polygons\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bounding box array from envelope\n",
    "    df = df.withColumn(\n",
    "        \"bbox\",\n",
    "        F.expr(f\"\"\"\n",
    "        array(\n",
    "            ST_X(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_Y(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_X(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_Y(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer})\n",
    "        )\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    # Turn bbox array into ST_Polygon\n",
    "    df = df.withColumn(\n",
    "        \"Polygons\",\n",
    "        F.expr(\"ST_MakeEnvelope(bbox[0], bbox[1], bbox[2], bbox[3])\")\n",
    "    )\n",
    "\n",
    "    # Apply adjusted BBOX logic using the UDF\n",
    "    df = df.withColumn(\n",
    "        \"adjusted_struct\",\n",
    "        adjusted_bbox_udf(F.col(\"bbox\"))\n",
    "    ).withColumn(\n",
    "        \"Adjusted_bbox\", F.col(\"adjusted_struct.bbox\")\n",
    "    ).withColumn(\n",
    "        \"bbox_str\", F.col(\"adjusted_struct.bbox_str\")\n",
    "    ).drop(\"envelope\")  # Drop envelope if no longer needed\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060571aa-5e5a-41a4-b814-ab3bb28bff3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dom_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.hoyde-dom-nhm-25833?request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=NHM_DOM_25833:skyggerelieff&\"\n",
    "        f\"BBOX={bbox_str}&width={width}&height={height}\"\n",
    "    )\n",
    "\n",
    "def generate_image_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.nib?VERSION=1.3.0\"\n",
    "        f\"&service=WMS&request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=ortofoto&\"\n",
    "        f\"BBox={bbox_str}&width={width}&height={height}&TICKET=\"\n",
    "    )\n",
    "\n",
    "def dom_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSdom/dom_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def image_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSimage/image_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def mask_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSlabel/mask_{id}.png\"\n",
    "    return \"GENERATED\" if os.path.exists(path) else \"PENDING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f83b64-e33a-442d-8d5e-323009a24000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BRUKERID = os.getenv(\"GEONORGE_BRUKERID\")\n",
    "PASSORD  = os.getenv(\"GEONORGE_PASSORD\")\n",
    "\n",
    "def get_token():\n",
    "    url = (\n",
    "        f\"https://baat.geonorge.no/skbaatts/req?brukerid={BRUKERID}\"\n",
    "        f\"&passord={PASSORD}&tjenesteid=wms.nib&retformat=s\"\n",
    "    )\n",
    "    raw_token = requests.get(url).text.strip(\"`\")\n",
    "    return raw_token\n",
    "\n",
    "token = get_token()\n",
    "token_start_time = time.time()\n",
    "token_lifetime = 55 * 60  # sekunder\n",
    "\n",
    "def refresh_token_if_needed():\n",
    "    global token, token_start_time\n",
    "    if time.time() - token_start_time > token_lifetime:\n",
    "        print(\"🔄 Fornyer token...\")\n",
    "        token = get_token()\n",
    "        token_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ceb352c-5d67-433b-ba37-0557080ee71c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_fotodato(bbox: str, token: str):\n",
    "    url = f\"https://wms.geonorge.no/skwms1/wms.nib?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&CRS=EPSG:25833&BBOX={bbox}&WIDTH=512&HEIGHT=512&LAYERS=ortofoto&QUERY_LAYERS=ortofoto&INFO_FORMAT=text/html&I=256&J=256&TICKET={token}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\")\n",
    "    field_value = None\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) >= 2 and cells[0].text.strip() == \"Fotodato\":\n",
    "                field_value = cells[1].text.strip()\n",
    "                field_value = datetime.strptime(field_value, \"%d.%m.%Y\").date()\n",
    "                return field_value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61746b7c-4efe-4bd3-957c-ce4b8fd02b63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ortofoto_date(df, token: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Add the ortofoto date to the dataframe based on bbox.\n",
    "    Returns a new DataFrame with the 'fototid' column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only relevant columns for processing\n",
    "    sample_rows = df.select(\"row_hash\", \"bbox_str\").collect()\n",
    "\n",
    "    # Fetch fotodato values from WMS\n",
    "    bbox_date_pairs = [\n",
    "        (row[\"row_hash\"], get_fotodato(row[\"bbox_str\"].replace('_', ','), token))\n",
    "        for row in sample_rows\n",
    "    ]\n",
    "\n",
    "    # Schema for intermediate DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"row_hash\", StringType(), True),\n",
    "        StructField(\"fototid\", DateType(), True)  # Use StringType if needed\n",
    "    ])\n",
    "\n",
    "    # Create small lookup DataFrame\n",
    "    bbox_date_df = spark.createDataFrame(bbox_date_pairs, schema)\n",
    "\n",
    "    # Join back on row_hash\n",
    "    df_with_date = df.join(bbox_date_df, on=\"row_hash\", how=\"left\")\n",
    "\n",
    "    return df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6914a658-0579-401f-9ecf-6bcbc2c0ba30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(\"source\"),\n",
    "                condition=\"target.row_hash = source.row_hash\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600bb3b3-1794-4a0d-9e90-8b4284c53896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def safe_load_wkt(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return wkt.loads(val)\n",
    "        except WKTReadingError:\n",
    "            print(\"❌ Hatalı WKT:\", val)\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_geopandas(df: DataFrame, column_name: str):\n",
    "    pdf = df.toPandas()\n",
    "    print(\"🔍 DataFrame Pandas'a dönüştürüldü.\")\n",
    "    print(\"Kolonlar:\", pdf.columns)\n",
    "    print(\"İlk 3 satır (WKT):\", pdf[column_name].head(3))\n",
    "\n",
    "    pdf[\"geometry\"] = pdf[column_name].apply(safe_load_wkt)\n",
    "    return gpd.GeoDataFrame(pdf, geometry=\"geometry\", crs=\"EPSG:25833\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705a95b8-c7a2-41e0-9662-74fdd7beddd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def dom_path(row_hash):\n",
    "    return f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSdom/dom_{row_hash}.png\"\n",
    "\n",
    "def image_path(row_hash):\n",
    "    return f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSimage/image_{row_hash}.png\"\n",
    "\n",
    "def mask_path(row_hash):\n",
    "    return f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSlabel/mask_{row_hash}.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36eddef6-8564-41a0-83c7-abb3b9783e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dom_path_udf = udf(dom_path, StringType())\n",
    "image_path_udf = udf(image_path, StringType())\n",
    "mask_path_udf = udf(mask_path, StringType())\n",
    "\n",
    "adjusted_bbox_schema = StructType([\n",
    "    StructField(\"bbox\", ArrayType(DoubleType())),\n",
    "    StructField(\"bbox_str\", StringType())\n",
    "])\n",
    "adjusted_bbox_udf = F.udf(lambda envelope: random_adjusted_bbox_centered(envelope), adjusted_bbox_schema)\n",
    "generate_dom_url_udf = F.udf(generate_dom_url, StringType())\n",
    "generate_image_url_udf = F.udf(generate_image_url, StringType())\n",
    "dom_file_exists_udf = F.udf(dom_file_exists, StringType())\n",
    "image_file_exists_udf = F.udf(image_file_exists, StringType())\n",
    "mask_file_exists_udf = F.udf(mask_file_exists, StringType())\n",
    "\n",
    "df= load_geometry_from_column()\n",
    "df = add_envolope_column(df)\n",
    "df = make_bbox(df)\n",
    "df = df.withColumn(\"dom_wms\", generate_dom_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"image_wms\", generate_image_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"dom_path\", dom_path_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"image_path\", image_path_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"mask_path\", mask_path_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"dom_status\", dom_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"image_status\", image_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"mask_status\", mask_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"lastet_tid\", F.current_timestamp())\n",
    "#df = add_ortofoto_date(df, token)\n",
    "gdf= to_geopandas(df, \"bbox\")\n",
    "write_delta_table(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utenSnuplass_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
