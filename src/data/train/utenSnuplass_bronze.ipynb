{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb71f55-f7f7-4d04-90e6-15a3b1cc8dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8253ecf-e402-4da3-a842-0c871d473027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gcs_landing_zone = \"/Volumes/land_topografisk-gdb_dev/external_dev/landing_zone/\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev= \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "log_table = \"logs_processed_gdbs_utensnuplass\"\n",
    "bronze_table= \"utensnuplass_bronze\"\n",
    "layer = \"ikke_snuplass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777bce9a-6c17-4746-afca-06516ae2e125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "   Shape_Length DOUBLE,\n",
    "   Shape_Area DOUBLE,\n",
    "   geometry STRING,\n",
    "   ingest_time TIMESTAMP,\n",
    "   source_file STRING,\n",
    "   source_layer STRING,\n",
    "   row_hash STRING\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f165ba-c05b-4e17-ae4d-a171ad53b7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  gdb_name STRING,\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5578aa6-5939-4dd6-85e1-afcb7b272e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_processed_gdb(log_data: list):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen. \n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"gdb_name\", StringType(), True),\n",
    "        StructField(\"processed_time\", TimestampType(), True),\n",
    "        StructField(\"num_inserted\", IntegerType(), True),\n",
    "        StructField(\"num_updated\", IntegerType(), True),\n",
    "        StructField(\"num_deleted\", IntegerType(), True)\n",
    "        ])\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\"append\").saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98e71de-cee7-419e-844d-b38b1945c6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_gdbs() -> list:\n",
    "    \"\"\"\n",
    "    Returnerer en liste med geodatabaser som ikke er lagret i deltatabellen.\n",
    "    \"\"\"\n",
    "    all_gdbs = [\n",
    "        f.path for f in dbutils.fs.ls(gcs_landing_zone) if f.path.endswith(\".gdb/\")\n",
    "    ]\n",
    "    processed_gdbs_df = spark.read.table(log_table).select(\"gdb_name\")\n",
    "    processed_gdbs = [row[\"gdb_name\"] for row in processed_gdbs_df.collect()]\n",
    "\n",
    "    return [gdb for gdb in all_gdbs if gdb not in processed_gdbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16df4137-20bd-4b55-8ee6-61aed90b1685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    \"\"\"\n",
    "    Konverterer geometri til WKT format i 2D format.\n",
    "    \"\"\"\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0d81fd-e073-4b32-9e3a-172c874d24cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(gdb_path: str, gdb_name: str, layer: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returnerer en spark dataframe med data fra deltatabellen.\n",
    "    \"\"\"\n",
    "    gdf = (\n",
    "        gpd.read_file(gdb_path, layer=layer).set_crs(\"EPSG:25833\").to_crs(\"EPSG:25833\")\n",
    "    )\n",
    "    gdf[\"wkt_geometry\"] = gdf[\"geometry\"].apply(to_wkt_2d)\n",
    "    gdf = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "    sdf = spark.createDataFrame(gdf)\n",
    "    sdf = sdf.withColumnRenamed(\"wkt_geometry\", \"geometry\")\n",
    "    sdf = (\n",
    "        sdf.withColumn(\"ingest_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(gdb_name))\n",
    "        .withColumn(\"source_layer\", lit(layer))\n",
    "        .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256))\n",
    "    )\n",
    "\n",
    "    target_cols = DeltaTable.forName(spark, bronze_table).toDF().columns\n",
    "    sdf = sdf.select([c for c in sdf.columns if c in target_cols])\n",
    "    sdf = sdf.dropDuplicates([\"row_hash\"])\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5ea27e-b530-46a6-a3d2-9c4aeb599219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver data til deltatabellen og opdaterer dersom den row_hash allerede finnes.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(bronze_table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c204687-8bb7-44ba-83e5-2a256ab476c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(sdf: DataFrame, gdb_name: str):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen og lagrer denne.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_processed_gdb(log_data=[(gdb_name, datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f939ed85-3060-4367-a708-c0636a27d597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Finner nye geodatabaser og skriver til deltatabellen.\n",
    "    \"\"\"\n",
    "    gdbs = check_for_new_gdbs()\n",
    "    for gdb in gdbs:\n",
    "        gdb_name = gdb.rstrip(\"/\").split(\"/\")[-1]\n",
    "        gdb_path = gdb.removeprefix(\"dbfs:\")\n",
    "        print(f\"\\nProcessing gdb: {gdb_name}\")\n",
    "\n",
    "        sdf = write_to_sdf(gdb_path, gdb_name, layer)\n",
    "        write_to_delta_table(sdf, gdb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577ee55e-f3de-4ba8-a93b-2f92327e70ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utenSnuplass_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
