{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fb71f55-f7f7-4d04-90e6-15a3b1cc8dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8253ecf-e402-4da3-a842-0c871d473027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gcs_landing_zone = \"/Volumes/land_topografisk-gdb_dev/external_dev/landing_zone/\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev= \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "log_table = \"logs_processed_gdbs\"\n",
    "bronze_table= \"utensnuplass_bronze\"\n",
    "layer = \"ikke_snuplass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "777bce9a-6c17-4746-afca-06516ae2e125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "   Shape_Length DOUBLE,\n",
    "   Shape_Area DOUBLE,\n",
    "   bbox STRING,\n",
    "   id STRING,\n",
    "   ingest_time TIMESTAMP,\n",
    "   row_hash STRING\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42f165ba-c05b-4e17-ae4d-a171ad53b7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  gdb_name STRING,\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5578aa6-5939-4dd6-85e1-afcb7b272e47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_processed_gdb(log_data: list):\n",
    "    \"\"\"\n",
    "    Writes the processed gdb to the log table.\n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"gdb_name\", StringType(), True),\n",
    "        StructField(\"processed_time\", TimestampType(), True),\n",
    "        StructField(\"num_inserted\", IntegerType(), True),\n",
    "        StructField(\"num_updated\", IntegerType(), True),\n",
    "        StructField(\"num_deleted\", IntegerType(), True)\n",
    "        ])\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\"append\").saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e98e71de-cee7-419e-844d-b38b1945c6d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_gdbs() -> list:\n",
    "    \"\"\"\n",
    "    Returns a lits of utensnuplass gdbs that have not been processed yet\n",
    "    \"\"\"\n",
    "    all_gdbs =[\n",
    "        f.path for f in dbutils.fs.ls(gcs_landing_zone)\n",
    "        if f.path.endswith(\".gdb/\") \n",
    "    ]\n",
    "\n",
    "    processed_gdbs_df=spark.read.table(log_table).select(\"gdb_name\")\n",
    "    processed_gdbs=[row[\"gdb_name\"] for row in processed_gdbs_df.collect()]\n",
    "\n",
    "    return [gdb for gdb in all_gdbs if gdb not in processed_gdbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16df4137-20bd-4b55-8ee6-61aed90b1685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    if geom and isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40eb6981-be38-480f-858b-3ebfc014e373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def multipolygon_to_polygon(geom):\n",
    "    if isinstance(geom, MultiPolygon):\n",
    "        largest = None\n",
    "        largest_area = -1\n",
    "        for poly in geom.geoms:\n",
    "            if poly.area > largest_area:\n",
    "                largest = poly\n",
    "                largest_area = poly.area\n",
    "        return largest\n",
    "    return geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc0d81fd-e073-4b32-9e3a-172c874d24cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(gdb_path: str, gdb_name: str, layer: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a .gdb layer and returns a minimal Spark DataFrame for bronze ingestion.\n",
    "    \"\"\"\n",
    "    gdf = gpd.read_file(gdb_path, layer=layer)\n",
    "    gdf = gdf.to_crs(\"EPSG:25833\")\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(multipolygon_to_polygon)\n",
    "    gdf[\"wkt_geometry\"] = gdf[\"geometry\"].apply(to_wkt_2d)\n",
    "    gdf = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "    sdf = spark.createDataFrame(gdf).withColumnRenamed(\"wkt_geometry\", \"bbox\")\n",
    "    sdf = (\n",
    "        sdf.withColumn(\"id\", expr(\"uuid()\"))\n",
    "        .withColumn(\"source_file\", lit(gdb_name))\n",
    "        .withColumn(\"ingest_time\", current_timestamp())\n",
    "    )\n",
    "    sdf = sdf.withColumn(\"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256))\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8356c0d3-31fd-4eef-b5a0-f6b9199eaf91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gdb_path = \"/Volumes/land_topografisk-gdb_dev/external_dev/landing_zone/Snuplasser0108.gdb/\"\n",
    "gdb_name = \"Snuplasser0108.gdb\"\n",
    "layer = \"ikke_snuplass\"\n",
    "\n",
    "df = write_to_sdf(gdb_path, gdb_name, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e5ea27e-b530-46a6-a3d2-9c4aeb599219",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(bronze_table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(bronze_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "                    source=sdf.alias(\"source\"),\n",
    "                    condition=\"target.row_hash = source.row_hash\"\n",
    "                ).whenMatchedUpdate(\n",
    "                    condition=\"target.row_hash != source.row_hash\",\n",
    "                    set={col: f\"source.{col}\" for col in sdf.columns}\n",
    "                ).whenNotMatchedInsert(\n",
    "                    values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "                ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c204687-8bb7-44ba-83e5-2a256ab476c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(sdf: DataFrame, gdb_name: str):\n",
    "    \"\"\"\n",
    "    Updates the delta table and logs the processed gdb.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "    \n",
    "    log_processed_gdb(log_data = [(gdb_name, datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f939ed85-3060-4367-a708-c0636a27d597",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Scans the landing zone for unprocessed .gdb files,\n",
    "    extracts the specified layer, writes the data to the Delta bronze table,\n",
    "    and logs the operation.\n",
    "    \"\"\"\n",
    "    # Specify the layer name within the .gdb file\n",
    "    layer = \"ikke_snuplass\"\n",
    "\n",
    "    # Get list of unprocessed .gdb files\n",
    "    gdbs = check_for_new_gdbs()\n",
    "\n",
    "\n",
    "    # Loop over each .gdb file\n",
    "    for gdb in gdbs:\n",
    "        gdb_name = gdb.rstrip(\"/\").split(\"/\")[-1]        # extract file name\n",
    "        gdb_path = gdb.removeprefix(\"dbfs:\")             # fix file path\n",
    "        print(f\"\\nüöß Processing: {gdb_name}\")\n",
    "\n",
    "        try:\n",
    "            # Convert the GDB layer to a Spark DataFrame\n",
    "            sdf = write_to_sdf(gdb_path, gdb_name, layer)\n",
    "\n",
    "            # Write data to Delta bronze table and log the operation\n",
    "            write_to_delta_table(sdf, gdb_name)\n",
    "\n",
    "            print(f\"‚úÖ Done: {gdb_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error while processing {gdb_name}: {e}\")\n",
    "    df_bronze = spark.read.table(bronze_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577ee55e-f3de-4ba8-a93b-2f92327e70ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utenSnuplass_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
