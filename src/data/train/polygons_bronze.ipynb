{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bea8fa2-0ab0-4a8c-bafa-fdb0873760c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855cf3a3-cc68-4a9b-a9e3-b3e0a972f49c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# config\n",
    "gcs_landing_zone = \"/Volumes/land_topografisk-gdb_dev/external_dev/landing_zone\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "log_table = f\"logs_processed_gdbs\"\n",
    "table = f\"polygons_bronze\"\n",
    "layer = \"Snuplasser_areal_N50\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262d0d9b-1f8b-4505-8c13-af3596d126a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  gdb_name STRING,\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    "\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "682f5cb9-4566-42db-89d9-a3bc68f61843",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_processed_gdb(log_data: list):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen. \n",
    "    \"\"\"\n",
    "    schema = StructType([\n",
    "        StructField(\"gdb_name\", StringType(), True),\n",
    "        StructField(\"processed_time\", TimestampType(), True),\n",
    "        StructField(\"num_inserted\", IntegerType(), True),\n",
    "        StructField(\"num_updated\", IntegerType(), True),\n",
    "        StructField(\"num_deleted\", IntegerType(), True)\n",
    "        ])\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\"append\").saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27ce277f-cc90-480f-93c8-f75607b1cd10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_gdbs() -> list:\n",
    "    \"\"\"\n",
    "    Returnerer en liste med geodatabaser som ikke er lagret i deltatabellen.\n",
    "    \"\"\"\n",
    "    all_gdbs = [\n",
    "        f.path for f in dbutils.fs.ls(gcs_landing_zone) if f.path.endswith(\".gdb/\")\n",
    "    ]\n",
    "    processed_gdbs_df = spark.read.table(log_table).select(\"gdb_name\")\n",
    "    processed_gdbs = [row[\"gdb_name\"] for row in processed_gdbs_df.collect()]\n",
    "\n",
    "    return [gdb for gdb in all_gdbs if gdb not in processed_gdbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7698dd0-febe-421d-bbef-a45a4508d1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    \"\"\"\n",
    "    Konverterer geometri til WKT format i 2D format.\n",
    "    \"\"\"\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db09411e-5998-4e67-94f7-b79808e6dad9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(gdb_path: str, gdb_name: str, layer: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returnerer en spark dataframe med data fra deltatabellen.\n",
    "    \"\"\"\n",
    "    gdf = (\n",
    "        gpd.read_file(gdb_path, layer=layer).set_crs(\"EPSG:32633\").to_crs(\"EPSG:25833\")\n",
    "    )\n",
    "    gdf[\"wkt_geometry\"] = gdf[\"geometry\"].apply(to_wkt_2d)\n",
    "    gdf = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "    sdf = spark.createDataFrame(gdf)\n",
    "    sdf = sdf.withColumnRenamed(\"wkt_geometry\", \"geometry\")\n",
    "    sdf = (\n",
    "        sdf.withColumn(\"ingest_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(gdb_name))\n",
    "        .withColumn(\"source_layer\", lit(layer))\n",
    "        .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256))\n",
    "    )\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56a42234-17dc-4daa-93f8-9363199573d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver data til deltatabellen og opdaterer dersom den row_hash allerede finnes.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e7e263f-931f-427b-92ba-726aef7e9fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(sdf: DataFrame, gdb_name: str):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen og lagrer denne.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(table):\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_processed_gdb(log_data=[(gdb_name, datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95d48b4e-c47b-4da6-be21-5b9d203e0ec1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Finner nye geodatabaser og skriver til deltatabellen.\n",
    "    \"\"\"\n",
    "    gdbs = check_for_new_gdbs()\n",
    "    for gdb in gdbs:\n",
    "        gdb_name = gdb.rstrip(\"/\").split(\"/\")[-1]\n",
    "        gdb_path = gdb.removeprefix(\"dbfs:\")\n",
    "        print(f\"\\nProcessing gdb: {gdb_name}\")\n",
    "\n",
    "        sdf = write_to_sdf(gdb_path, gdb_name, layer)\n",
    "        write_to_delta_table(sdf, gdb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14fa746e-0b85-4d94-9d1c-c068de7461dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4990219242786202,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "polygons_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
