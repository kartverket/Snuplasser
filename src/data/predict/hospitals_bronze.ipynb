{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "343e5909-6921-437d-a8a4-1722730a84cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a283069-8143-4a42-864c-5c2ac03091d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "gcs_landing_zone = \"/Volumes/land_auto-gen-kart_dev/external_dev/landing_zone\"\n",
    "catalog_dev = \"`land_auto-gen-kart_dev`\"\n",
    "schema_dev = \"dl_bildesegmentering\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "bronze_table = \"hospitals_bronze\"\n",
    "log_table = \"logs_processed_hospital_gdbs\"\n",
    "layer = \"BygningsPunkt_Sykehus_med_akuttmottak\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70d9e676-3fdc-488c-bc4b-b33c11a5809e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {bronze_table} (\n",
    "    row_hash STRING,\n",
    "    source_file STRING,\n",
    "    source_layer STRING,\n",
    "    geometry BINARY,\n",
    "    ingest_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc2a98c9-881b-4cb1-b07c-78d60392a651",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  gdb_name STRING,\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b3da1f-9ca3-4165-8a82-6c3b48f38310",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_processed_gdb(log_data: list):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen.\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"gdb_name\", StringType(), True),\n",
    "            StructField(\"processed_time\", TimestampType(), True),\n",
    "            StructField(\"num_inserted\", IntegerType(), True),\n",
    "            StructField(\"num_updated\", IntegerType(), True),\n",
    "            StructField(\"num_deleted\", IntegerType(), True),\n",
    "        ]\n",
    "    )\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\n",
    "        \"append\"\n",
    "    ).saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f9081d-a531-4c42-8116-f1c2909692f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_gdbs() -> list:\n",
    "    \"\"\"\n",
    "    Returnerer en liste med geodatabaser som ikke er lagret i deltatabellen.\n",
    "    \"\"\"\n",
    "    all_gdbs = [\n",
    "        f.path for f in dbutils.fs.ls(gcs_landing_zone) if f.path.endswith(\"Sykehus.gdb/\")\n",
    "    ]\n",
    "    processed_gdbs_df = spark.read.table(log_table).select(\"gdb_name\")\n",
    "    processed_gdbs = [row[\"gdb_name\"] for row in processed_gdbs_df.collect()]\n",
    "\n",
    "    return [gdb for gdb in all_gdbs if gdb not in processed_gdbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ce25d7-395b-4433-b423-da97c2962d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    \"\"\"\n",
    "    Konverterer geometri til WKT format i 2D format.\n",
    "    \"\"\"\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3daf1224-ec70-4fca-ba60-baf4f2b4ee71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(gdb_path: str, gdb_name: str, layer: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returnerer en spark dataframe med data fra deltatabellen.\n",
    "    \"\"\"\n",
    "    gdf = (\n",
    "        gpd.read_file(gdb_path, layer=layer).set_crs(\"EPSG:25833\").to_crs(\"EPSG:25833\")\n",
    "    )\n",
    "    gdf[\"wkt_geometry\"] = gdf[\"geometry\"].apply(to_wkt_2d)\n",
    "    gdf = gdf.drop(columns=[\"geometry\"])\n",
    "\n",
    "    sdf = spark.createDataFrame(gdf)\n",
    "    sdf = sdf.withColumnRenamed(\"wkt_geometry\", \"geometry\")\n",
    "    sdf = (\n",
    "        sdf.withColumn(\"ingest_time\", current_timestamp())\n",
    "        .withColumn(\"source_file\", lit(gdb_name))\n",
    "        .withColumn(\"source_layer\", lit(layer))\n",
    "        .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256))\n",
    "    )\n",
    "\n",
    "    target_cols = DeltaTable.forName(spark, bronze_table).toDF().columns\n",
    "    sdf = sdf.select([c for c in sdf.columns if c in target_cols])\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5120a0a7-ca05-47cf-accc-c5348c20180b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame, mode: str = \"merge\") -> None:\n",
    "    \"\"\"\n",
    "    Skriver data til deltatabellen og oppdaterer dersom row_hash allerede finnes.\n",
    "    \"\"\"\n",
    "    if mode == \"overwrite\":\n",
    "        sdf.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\n",
    "            \"overwrite\"\n",
    "        ).saveAsTable(bronze_table)\n",
    "    else:\n",
    "        from delta.tables import DeltaTable\n",
    "\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.ingest_time < source.ingest_time\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9156415-ec74-40a2-adaa-fdff63802c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(sdf: DataFrame, gdb_name: str):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen og lagrer denne.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(bronze_table):\n",
    "        delta_tbl = DeltaTable.forName(spark, bronze_table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_processed_gdb(log_data=[(gdb_name, datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a46d90cf-4f5d-4241-89d3-c0637e1bbead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Finner nye geodatabaser og skriver til deltatabellen.\n",
    "    \"\"\"\n",
    "    gdbs = check_for_new_gdbs()\n",
    "    for gdb in gdbs:\n",
    "        gdb_name = gdb.rstrip(\"/\").split(\"/\")[-1]\n",
    "        gdb_path = gdb.removeprefix(\"dbfs:\")\n",
    "        print(f\"\\nProcessing gdb: {gdb_name}\")\n",
    "\n",
    "        sdf = write_to_sdf(gdb_path, gdb_name, layer)\n",
    "        write_to_delta_table(sdf, gdb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a10d135-0ff6-4ab8-beab-fb529019e59f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "hospitals_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
