{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc28474-dadc-4056-ac25-049969b9372e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-image==0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf586b-d682-42a1-897f-70efc5345130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c07d4b-3b49-4d39-9044-495fc60de30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_masks = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_masks\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f'USE CATALOG {catalog_dev}')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_dev}')\n",
    "spark.sql(f'USE SCHEMA {schema_dev}')\n",
    "log_table = \"logs_predicted_masks\"\n",
    "bronze_table = \"predicted_bronze\"\n",
    "silver_table = \"predicted_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dc164bb-5dee-4bbd-ab42-26afb9649edf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_turning_spaces() -> list[Row]:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with all turning spaces.\n",
    "    \"\"\"\n",
    "    df_turning_spaces = spark.read.table(bronze_table).filter(F.col(\"turning_space\"))\n",
    "    return df_turning_spaces.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c171380-b9e0-4fd5-8e47-86cc3b50fd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_bbox_from_table(nodeid: str) -> DataFrame:\n",
    "    df = spark.read.table(bronze_table).filter(F.col(\"nodeid\") == nodeid).select(F.col(\"bbox\")).first().bbox\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1149c33e-3bca-43c1-94a4-6b04f1579161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_to_gdf(mask: Row) -> tuple[gpd.GeoDataFrame, str]:\n",
    "    source_file = mask.source_file\n",
    "    bbox = read_bbox_from_table(source_file[11:-4])\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    mask_image = Image.open(f\"{predicted_masks}/{source_file}\").convert(\"L\")\n",
    "    width, height = mask_image.size  \n",
    "    x_res = (x_max - x_min) / width\n",
    "    y_res = (y_max - y_min) / height\n",
    "\n",
    "    mask = np.array(mask_image)\n",
    "    mask_bin = (mask > 127).astype(np.uint8)\n",
    "\n",
    "    contours = measure.find_contours(mask_bin, 0.5)\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        coords = []\n",
    "        for y, x in contour:\n",
    "            x_coord = x_min + x * x_res\n",
    "            y_coord = y_max - y * y_res\n",
    "            coords.append((x_coord, y_coord))\n",
    "        poly = Polygon(coords)\n",
    "        if poly.is_valid:\n",
    "            polygons.append(poly)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=polygons, crs=\"EPSG:25833\")\n",
    "    return gdf, source_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cbe5e9-523a-464c-8da2-2ef1b89e789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_diameter(\n",
    "    df: DataFrame,\n",
    "    wkt_col: str = \"geometry\",\n",
    "    id_col: str = \"row_hash\",\n",
    "    out_col: str = \"diameter\"\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column to df giving the minimum caliper width (shortest \n",
    "    distance across the largest continuous part of each polygon).\n",
    "    \"\"\"\n",
    "    @udf(returnType=DoubleType())\n",
    "    def _min_caliper_width(wkt_str: str) -> float:\n",
    "        geom = wkt.loads(wkt_str)\n",
    "\n",
    "        # Største sammenhengende polygon\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = sorted(geom.geoms, key=lambda g: g.area)[-1]\n",
    "\n",
    "        # Convex hull for enklere beregninger\n",
    "        hull = geom.convex_hull\n",
    "        coords = list(hull.exterior.coords[:-1])  # Fjerner siste punkt som er likt det første\n",
    "\n",
    "        # Beregner alle perpendikulære avstander fra et punkt til motsatt kant\n",
    "        min_width = float(\"inf\")\n",
    "        for i in range(len(coords)):\n",
    "            a, b = coords[i], coords[(i + 1) % len(coords)]\n",
    "            edge_dx = b[0] - a[0]\n",
    "            edge_dy = b[1] - a[1]\n",
    "            length = (edge_dx**2 + edge_dy**2)**0.5\n",
    "            if length == 0:\n",
    "                continue\n",
    "\n",
    "            # Enhetsvektor perpendikulær til kant\n",
    "            perp_dx, perp_dy = -edge_dy / length, edge_dx / length\n",
    "\n",
    "            # Projiserer alle punkter til den perpendikulære vektoren\n",
    "            projections = [p[0]*perp_dx + p[1]*perp_dy for p in coords]\n",
    "            width = sorted(projections)[-1] - sorted(projections)[0] # Blir det samme som max(projections) - min(projections)\n",
    "            min_width = sorted([min_width, width])[0] # Blir det samme som min(min_width, width)\n",
    "        return float(min_width)\n",
    "    return df.withColumn(out_col, _min_caliper_width(F.col(wkt_col)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9476edc7-5ac7-4cf9-8443-42809ec0a89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def to_wkt_2d(geom):\n",
    "    if isinstance(geom, BaseGeometry):\n",
    "        return force_2d(geom).wkt\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0835fabb-40ff-498a-97a1-01b100bdb74d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(mask: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read GeoJSON and write one merged MultiPolygon row to SDF with centroid.\n",
    "    \"\"\"\n",
    "    gdf, mask_name = mask_to_gdf(mask)\n",
    "    \n",
    "    # Merge all geometries into a single MultiPolygon\n",
    "    merged_geom = unary_union(gdf.geometry)\n",
    "    if merged_geom.geom_type == \"Polygon\":\n",
    "        merged_geom = MultiPolygon([merged_geom])\n",
    "\n",
    "\n",
    "    # Calculate centroid\n",
    "    centroid = merged_geom.centroid\n",
    "    centroid_x = centroid.x\n",
    "    centroid_y = centroid.y\n",
    "\n",
    "    # Create single-row DataFrame\n",
    "    df = pd.DataFrame(\n",
    "        [{\"geometry\": wkt.dumps(merged_geom), \"centroid_x\": centroid_x, \"centroid_y\": centroid_y}]\n",
    "    )\n",
    "    basic_sdf = spark.createDataFrame(df)\n",
    "\n",
    "    sdf_diameter = calculate_diameter(basic_sdf)\n",
    "    sdf_clean = sdf_diameter.drop(\"geometry\") # Bruker ikke geometry etter dette\n",
    "\n",
    "    # Add metadata\n",
    "    sdf = (\n",
    "        sdf_clean.withColumn(\"ingest_time\", F.current_timestamp())\n",
    "        .withColumn(\"source_file\", F.lit(mask_name))\n",
    "        .withColumn(\"row_hash\", F.sha2(F.concat_ws(\"||\", *sdf_clean.columns), 256))\n",
    "    )\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db6c44b-4f49-4b7f-9883-53cfce0c6eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc345f-0cf2-4d77-990a-f1d1b7efac90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Reads predicted masks and writes them to delta table.\n",
    "    \"\"\"\n",
    "    predicted_masks = get_turning_spaces()\n",
    "    for mask in predicted_masks:\n",
    "        sdf = write_to_sdf(mask)\n",
    "        write_delta_table(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe83d20-7173-4873-8dd9-146265446fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
