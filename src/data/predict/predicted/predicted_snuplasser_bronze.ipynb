{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "552eac75-9ebd-4eb3-857e-0593e1cd6ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pyproj rasterio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cce8d9-7437-47c4-9760-858e6527312c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from shapely import wkt\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import GeometryCollection, Polygon, MultiPolygon\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    StringType,\n",
    ")\n",
    "\n",
    "from src.data.data_utils import write_delta_table\n",
    "from src.data.log_utils import check_for_new_predicted_masks, log_predicted_masks\n",
    "from src.data.transformation_utils import get_srid, transform_to_epsg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edef3876-8d9c-4c29-a940-52842d3e37a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mask_path = \"/Volumes/land_auto-gen-kart_dev/external_dev/static_data/DL_bildesegmentering/predicted_snuplasser\"\n",
    "catalog_dev = \"`land_auto-gen-kart_dev`\"\n",
    "schema_dev = \"dl_bildesegmentering\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "log_table = \"logs_predicted_snuplasser\"\n",
    "table = \"predicted_snuplasser_bronze\"\n",
    "endepunkt_silver_table = \"endepunkt_silver\"\n",
    "\n",
    "land_catalog = \"land_ngis_dev\"\n",
    "bygning_schema = \"silver_fkbbygning\"\n",
    "bygning_table = \"bygning\"\n",
    "dataset = f\"{land_catalog}.{bygning_schema}.{bygning_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2564f5ec-c131-46c6-8292-d02ee3944254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49678751-fb9a-4187-82ba-3f8de613f3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table} (\n",
    "    row_hash STRING,\n",
    "    white_pixels INT,\n",
    "    turning_space BOOLEAN,\n",
    "    bbox ARRAY<DOUBLE>,\n",
    "    source_file STRING,\n",
    "    ingest_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22689022-6aaa-4c84-9475-de7c12751107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_buildings(bbox: str, polygons: List) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returnerer et array med bygninger i bboxen.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    out_shape = (369, 369)\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, out_shape[1], out_shape[0])\n",
    "\n",
    "    # Rasteriserer polygonene til et raster med samme størrelse som masken.\n",
    "    geoms = [poly for poly in polygons]\n",
    "    mask = rasterize(\n",
    "        [(geom, 1) for geom in geoms],\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=\"uint8\",\n",
    "    )\n",
    "    arr = np.array(mask)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a25164-763d-48f3-a556-ee33753c08ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_geometries(geom):\n",
    "    \"\"\"\n",
    "    Returnerer en liste med geometrier som kan brukes til å rasterize masken.\n",
    "    \"\"\"\n",
    "    if geom is None:\n",
    "        return []\n",
    "    elif isinstance(geom, (Polygon, MultiPolygon)):\n",
    "        return [geom]\n",
    "    elif isinstance(geom, GeometryCollection):\n",
    "        # recursively extract polygons / multipolygons\n",
    "        geoms = []\n",
    "        for g in geom.geoms:\n",
    "            geoms.extend(flatten_geometries(g))\n",
    "        return geoms\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937987c2-e091-49f6-95f2-db2a70336632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(predicted_masks: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returnerer en spark dataframe med data fra deltatabellen.\n",
    "    \"\"\"\n",
    "    # Lager en spark dataframe fra maskene\n",
    "    df = spark.createDataFrame([(m,) for m in predicted_masks], [\"mask\"])\n",
    "    df = df.withColumn(\"nodeid\", expr(\"substring(mask, 12, length(mask)-15)\"))\n",
    "\n",
    "    silver = (\n",
    "        spark.read.table(endepunkt_silver_table)\n",
    "        .withColumn(\n",
    "            \"bbox_geom\",\n",
    "            expr(\n",
    "                \"\"\"\n",
    "                ST_PolygonFromEnvelope(\n",
    "                    bbox[0], bbox[1], bbox[2], bbox[3]\n",
    "                )\n",
    "            \"\"\"\n",
    "            ),\n",
    "        )\n",
    "        .select(\"nodeid\", \"geometry\", \"kommune_id\", \"bbox\", \"bbox_geom\")\n",
    "    )\n",
    "\n",
    "    buildings = (\n",
    "        spark.read.table(dataset)\n",
    "        .select(\"kommunenummer\", \"geometry\")\n",
    "        .withColumnRenamed(\"geometry\", \"building_geometry\")\n",
    "    )\n",
    "    buildings = transform_to_epsg(\n",
    "        buildings,\n",
    "        col=\"building_geometry\",\n",
    "        source_srid=get_srid(land_catalog, bygning_schema, bygning_table),\n",
    "        target_srid=\"EPSG:25833\",\n",
    "    )\n",
    "\n",
    "    joined_all = (\n",
    "        df.join(silver, \"nodeid\")\n",
    "        .join(buildings, col(\"kommune_id\") == col(\"kommunenummer\"), \"left\")\n",
    "        .withColumn(\"intersects\", expr(\"ST_Intersects(bbox_geom, building_geometry)\"))\n",
    "    )\n",
    "\n",
    "    grouped_df = joined_all.groupBy(\"nodeid\", \"bbox\", \"mask\").agg(\n",
    "        collect_list(\n",
    "            when(col(\"intersects\"), expr(\"ST_AsText(building_geometry)\"))\n",
    "        ).alias(\"building_wkts\")\n",
    "    )\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"white_pixels\", IntegerType(), False),\n",
    "            StructField(\"turning_space\", BooleanType(), False),\n",
    "            StructField(\"bbox\", ArrayType(IntegerType()), False),\n",
    "            StructField(\"source_file\", StringType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def process_partition(pdf_iter):\n",
    "        for pdf in pdf_iter:\n",
    "            out_rows = []\n",
    "            for _, row in pdf.iterrows():\n",
    "                polygons = [wkt.loads(g) for g in row[\"building_wkts\"] if g]\n",
    "                merged = unary_union(polygons) if polygons else None\n",
    "                geoms = flatten_geometries(merged)\n",
    "\n",
    "                arr = np.array(Image.open(f\"{mask_path}/{row['mask']}\").convert(\"L\"))\n",
    "\n",
    "                inv_arr = get_buildings(row[\"bbox\"], geoms)\n",
    "                result_arr = np.clip(\n",
    "                    arr.astype(int) - inv_arr.astype(int), 0, 255\n",
    "                ).astype(np.uint8)\n",
    "\n",
    "                count_255 = int((result_arr == 255).sum())\n",
    "\n",
    "                out_rows.append(\n",
    "                    {\n",
    "                        \"white_pixels\": count_255,\n",
    "                        \"turning_space\": count_255 > 0,\n",
    "                        \"bbox\": row[\"bbox\"],\n",
    "                        \"source_file\": row[\"mask\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            yield pd.DataFrame(out_rows)\n",
    "\n",
    "    records_df = grouped_df.mapInPandas(process_partition, schema)\n",
    "    sdf = records_df.withColumn(\n",
    "        \"row_hash\", sha2(concat_ws(\"||\", *records_df.columns), 256)\n",
    "    ).withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe7cc1c6-f83f-4043-878c-65cc3a772a07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(predicted_masks: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen og lagrer denne.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(table):\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    if predicted_masks:\n",
    "        sdf = write_to_sdf(predicted_masks)\n",
    "        write_delta_table(sdf, table, id_col=\"row_hash\")\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            inserted, updated, deleted = 0, 0, 0\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_predicted_masks([(datetime.now(), inserted, updated, deleted)], log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b740c69b-9329-40a2-8d21-7dacf64bfa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_masks = check_for_new_predicted_masks(mask_path, table)\n",
    "write_to_delta_table(predicted_masks)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_snuplasser_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
