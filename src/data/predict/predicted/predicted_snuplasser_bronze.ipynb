{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cce8d9-7437-47c4-9760-858e6527312c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    TimestampType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    "    BooleanType,\n",
    "    StringType,\n",
    ")\n",
    "from pyproj import CRS\n",
    "from rasterio.features import rasterize\n",
    "from rasterio.transform import from_bounds\n",
    "from typing import Iterator, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edef3876-8d9c-4c29-a940-52842d3e37a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mask_path = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_snuplasser\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "log_table = \"logs_predicted_snuplasser\"\n",
    "table = \"predicted_snuplasser_bronze\"\n",
    "endepunkt_silver_table = \"endepunkt_silver\"\n",
    "\n",
    "land_catalog = \"land_ngis_dev\"\n",
    "bygning_schema = \"silver_fkbbygning\"\n",
    "bygning_table = \"bygning\"\n",
    "dataset = f\"{land_catalog}.{bygning_schema}.{bygning_table}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2564f5ec-c131-46c6-8292-d02ee3944254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49678751-fb9a-4187-82ba-3f8de613f3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table} (\n",
    "    row_hash STRING,\n",
    "    white_pixels INT,\n",
    "    turning_space BOOLEAN,\n",
    "    bbox ARRAY<DOUBLE>,\n",
    "    source_file STRING,\n",
    "    ingest_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3bb5fb-1dfe-4cb7-8492-15c0c862e662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sett kontekst med katalog og skjema\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eafecc1-4635-4234-9e99-fac109c4f122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_predicted_mask(log_data: list):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen.\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"processed_time\", TimestampType(), True),\n",
    "            StructField(\"num_inserted\", IntegerType(), True),\n",
    "            StructField(\"num_updated\", IntegerType(), True),\n",
    "            StructField(\"num_deleted\", IntegerType(), True),\n",
    "        ]\n",
    "    )\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\n",
    "        \"append\"\n",
    "    ).saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb08fa4-246e-431b-acb7-d9b7e19f422b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_predicted_masks() -> list:\n",
    "    \"\"\"\n",
    "    Returnerer en liste med masker som ikke er lagret i deltatabellen.\n",
    "    \"\"\"\n",
    "    all_masks = [\n",
    "        f.path.rstrip(\"/\").split(\"/\")[-1]\n",
    "        for f in dbutils.fs.ls(mask_path)\n",
    "        if f.path.endswith(\".png\")\n",
    "    ]\n",
    "\n",
    "    processed_masks_df = spark.read.table(table).select(\"source_file\")\n",
    "    processed_masks = [row[\"source_file\"] for row in processed_masks_df.collect()]\n",
    "\n",
    "    return [mask for mask in all_masks if mask not in processed_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbe25069-f2de-4772-88a0-bc07b76536a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_srid():\n",
    "    \"\"\"\n",
    "    Henter ut SRID fra metadata tags.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT tag_value\n",
    "        FROM system.information_schema.table_tags\n",
    "        WHERE catalog_name = '{land_catalog}'\n",
    "          AND schema_name = '{bygning_schema}'\n",
    "          AND table_name = '{bygning_table}'\n",
    "          AND tag_name = 'SRID'\n",
    "    \"\"\"\n",
    "    result = spark.sql(query).collect()\n",
    "    return result[0][\"tag_value\"] if result else None\n",
    "\n",
    "\n",
    "def crs_is_righthanded(srid: str) -> bool:\n",
    "    \"\"\"\n",
    "    Sjekker om koordinatsystemet er høyrehåndsystem.\n",
    "    Dvs. rekkefølge øst, nord\n",
    "    \"\"\"\n",
    "    return CRS(srid).axis_info[0].direction.upper() == \"EAST\"\n",
    "\n",
    "\n",
    "def transform_to_epsg(\n",
    "    df: DataFrame,\n",
    "    col: str = \"geometry\",\n",
    "    source_srid: str = \"EPSG:5942\",\n",
    "    target_srid: str = \"EPSG:25833\",\n",
    ") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Transformerer geometri til ønsket EPSG-projeksjon.\n",
    "    \"\"\"\n",
    "\n",
    "    # Flippar koordinatane x og y viss koordinatsystemet er venstrehandsystem.\n",
    "    if crs_is_righthanded(source_srid) == False:\n",
    "        df = df.withColumn(col, expr(f\"ST_FlipCoordinates(ST_GeomFromWKB({col}))\"))\n",
    "\n",
    "    # Transformerer frå source_srid til target_srid (EPSG)\n",
    "    df = df.withColumn(\n",
    "        col, expr(f\"ST_Transform(geometry, '{source_srid}', '{target_srid}')\")\n",
    "    )\n",
    "\n",
    "    # Flippar koordinatane tilbake viss dei transformerte koordinatane er venstrehandsystem.\n",
    "    if crs_is_righthanded(target_srid) == False:\n",
    "        df = df.withColumn(col, expr(f\"ST_FlipCoordinates({col})\"))\n",
    "\n",
    "    df = df.select(col, \"kommunenummer\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22689022-6aaa-4c84-9475-de7c12751107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_buildings(bbox: str, polygons: List) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returnerer et array med bygninger i bboxen.\n",
    "    \"\"\"\n",
    "    minx, miny, maxx, maxy = bbox\n",
    "    out_shape = (369, 369)\n",
    "    transform = from_bounds(minx, miny, maxx, maxy, out_shape[1], out_shape[0])\n",
    "\n",
    "    # Rasteriserer polygonene til et raster med samme størrelse som masken.\n",
    "    geoms = [poly for poly in polygons]\n",
    "    print(geoms)\n",
    "    mask = rasterize(\n",
    "        [(geom, 1) for geom in geoms],\n",
    "        out_shape=out_shape,\n",
    "        transform=transform,\n",
    "        fill=0,\n",
    "        dtype=\"uint8\",\n",
    "    )\n",
    "    arr = np.array(mask)\n",
    "\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937987c2-e091-49f6-95f2-db2a70336632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame, functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType,\n",
    "    StructField,\n",
    "    StringType,\n",
    "    BooleanType,\n",
    "    IntegerType,\n",
    "    ArrayType,\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def write_to_sdf(predicted_masks: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Returnerer en spark dataframe med data fra deltatabellen.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Create a DataFrame of masks\n",
    "    df = spark.createDataFrame([(m,) for m in predicted_masks], [\"mask\"])\n",
    "\n",
    "    # Step 2: Join with silver table to get bbox + kommune_id\n",
    "    silver = spark.read.table(endepunkt_silver_table).select(\n",
    "        \"nodeid\", \"bbox\", \"kommune_id\"\n",
    "    )\n",
    "    buildings = spark.read.table(dataset).select(\"kommunenummer\", \"geometry\")\n",
    "    buildings = transform_to_epsg(\n",
    "        buildings, col=\"geometry\", source_srid=get_srid(), target_srid=\"EPSG:25833\"\n",
    "    )\n",
    "\n",
    "    df = (\n",
    "        spark.createDataFrame([(m,) for m in predicted_masks], [\"mask\"])\n",
    "        .withColumn(\"nodeid\", F.expr(\"substring(mask, 12, length(mask)-15)\"))\n",
    "        .join(silver, \"nodeid\")\n",
    "        .join(\n",
    "            buildings, F.col(\"kommune_id\") == F.col(\"kommunenummer\"), \"left\"\n",
    "        )  # keep all rows\n",
    "        .withColumn(\n",
    "            \"intersects\",\n",
    "            F.expr(\n",
    "                \"ST_Intersects(geometry, ST_PolygonFromEnvelope(bbox[0], bbox[1], bbox[2], bbox[3]))\"\n",
    "            ),\n",
    "        )\n",
    "        .groupBy(\"mask\", \"bbox\", \"kommune_id\")\n",
    "        .agg(\n",
    "            F.collect_list(F.when(F.col(\"intersects\"), F.col(\"geometry\"))).alias(\n",
    "                \"geoms\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"white_pixels\", IntegerType(), False),\n",
    "            StructField(\"turning_space\", BooleanType(), False),\n",
    "            StructField(\"bbox\", ArrayType(IntegerType()), False),\n",
    "            StructField(\"source_file\", StringType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def process_masks(iterator):\n",
    "        for pdf in iterator:\n",
    "            out_rows = []\n",
    "            for _, row in pdf.iterrows():\n",
    "                mask = row[\"mask\"]\n",
    "                bbox = row[\"bbox\"]\n",
    "                polygons = row[\"geoms\"]\n",
    "                arr = np.array(Image.open(f\"{mask_path}/{mask}\").convert(\"L\"))\n",
    "                inv_arr = get_buildings(bbox, polygons)\n",
    "                print(inv_arr)\n",
    "                result_arr = np.clip(\n",
    "                    arr.astype(int) - inv_arr.astype(int), 0, 255\n",
    "                ).astype(np.uint8)\n",
    "                count_255 = int((result_arr == 255).sum())\n",
    "                out_rows.append(\n",
    "                    {\n",
    "                        \"white_pixels\": count_255,\n",
    "                        \"turning_space\": bool(count_255 > 0),\n",
    "                        \"bbox\": bbox,\n",
    "                        \"source_file\": mask,\n",
    "                    }\n",
    "                )\n",
    "            yield pd.DataFrame(out_rows)\n",
    "\n",
    "    records_df = df.mapInPandas(process_masks, schema)\n",
    "\n",
    "    sdf = records_df.withColumn(\n",
    "        \"row_hash\", F.sha2(F.concat_ws(\"||\", *records_df.columns), 256)\n",
    "    ).withColumn(\"ingest_time\", F.current_timestamp())\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44b044d-df56-4205-9c9b-25801c6adf0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver data til deltatabellen og oppdaterer dersom row_hash allerede finnes.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2caa379-fe3b-4b19-946a-c9bc552d7d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(predicted_masks: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver logg med antall insert, update og deleter i deltatabellen og lagrer denne.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(table):\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    if predicted_masks:\n",
    "        sdf = write_to_sdf(predicted_masks)\n",
    "        write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            inserted, updated, deleted = 0, 0, 0\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_predicted_mask(log_data=[(datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b740c69b-9329-40a2-8d21-7dacf64bfa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Finner nye masker og skriver til deltatabellen.\n",
    "    \"\"\"\n",
    "    predicted_masks = check_for_new_predicted_masks()\n",
    "    write_to_delta_table(predicted_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203f4fb1-4776-4efb-aa30-89e5b9aa7714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_snuplasser_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
