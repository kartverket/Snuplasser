{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fc28474-dadc-4056-ac25-049969b9372e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install scikit-image==0.20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ecf586b-d682-42a1-897f-70efc5345130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely.ops import unary_union\n",
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "from shapely.geometry.base import BaseGeometry\n",
    "from shapely import force_2d\n",
    "from shapely import wkt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage import measure\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.sql.types import StructField, StringType, DoubleType\n",
    "from delta.tables import DeltaTable\n",
    "from typing import Tuple, Iterator\n",
    "from sedona.spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9c07d4b-3b49-4d39-9044-495fc60de30a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "predicted_masks = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_snuplasser\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "bronze_table = \"predicted_snuplasser_bronze\"\n",
    "silver_table = \"predicted_snuplasser_silver\"\n",
    "endepunkt_silver_table = \"endepunkt_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95aad31d-7689-4d51-a8c1-2734b3e4339f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {silver_table} (\n",
    "    row_hash STRING,\n",
    "    centroid_x DOUBLE,\n",
    "    centroid_y DOUBLE,\n",
    "    diameter DOUBLE,\n",
    "    fototid TIMESTAMP,\n",
    "    source_file STRING,\n",
    "    ingest_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1149c33e-3bca-43c1-94a4-6b04f1579161",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mask_to_gdf(bbox: str, source_file: str) -> tuple[gpd.GeoDataFrame, str]:\n",
    "    \"\"\"\n",
    "    Laster inn masken for en gitt source_file, og returnerer en GeoDataFrame med polygonene.\n",
    "    \"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    mask_image = Image.open(f\"{predicted_masks}/{source_file}\").convert(\"L\")\n",
    "    width, height = mask_image.size\n",
    "    x_res = (x_max - x_min) / width\n",
    "    y_res = (y_max - y_min) / height\n",
    "\n",
    "    mask = np.array(mask_image)\n",
    "    mask_bin = (mask > 127).astype(np.uint8)\n",
    "\n",
    "    contours = measure.find_contours(mask_bin, 0.5)\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        coords = []\n",
    "        for y, x in contour:\n",
    "            x_coord = x_min + x * x_res\n",
    "            y_coord = y_max - y * y_res\n",
    "            coords.append((x_coord, y_coord))\n",
    "        poly = Polygon(coords)\n",
    "        if poly.is_valid:\n",
    "            polygons.append(poly)\n",
    "\n",
    "    gdf = gpd.GeoDataFrame(geometry=polygons, crs=\"EPSG:25833\")\n",
    "    return gdf, source_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2895f2-a0cb-4fcd-8c61-0455b7f677c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def masks_grouped_udf(iterator):\n",
    "    \"\"\"\n",
    "    For hver gruppe leses alle maskene og polygonene slås sammen.\n",
    "    \"\"\"\n",
    "    for pdf in iterator:\n",
    "        groups = {}\n",
    "        for _, row in pdf.iterrows():\n",
    "            bbox = row[\"bbox\"]\n",
    "            source_file = row[\"source_file\"]\n",
    "            fototid = row.get(\"fototid\", None)\n",
    "            fototid = fototid.isoformat() if fototid is not None else None\n",
    "\n",
    "            try:\n",
    "                gdf, _ = mask_to_gdf(bbox, source_file)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if gdf is None or gdf.empty:\n",
    "                continue\n",
    "\n",
    "            merged_geom_for_file = unary_union(list(gdf.geometry))\n",
    "            if merged_geom_for_file is None or merged_geom_for_file.is_empty:\n",
    "                continue\n",
    "\n",
    "            key = (source_file, fototid)\n",
    "            groups.setdefault(key, []).append(merged_geom_for_file)\n",
    "\n",
    "        out_rows = []\n",
    "        for (source_file, fototid), geom_list in groups.items():\n",
    "            try:\n",
    "                merged_all = unary_union(geom_list)\n",
    "                if merged_all is None or merged_all.is_empty:\n",
    "                    continue\n",
    "                centroid = merged_all.centroid\n",
    "                out_rows.append(\n",
    "                    {\n",
    "                        \"source_file\": source_file,\n",
    "                        \"fototid\": fototid,\n",
    "                        \"geometry_wkt\": merged_all.wkt,\n",
    "                        \"centroid_x\": float(centroid.x)\n",
    "                        if centroid and not centroid.is_empty\n",
    "                        else float(\"nan\"),\n",
    "                        \"centroid_y\": float(centroid.y)\n",
    "                        if centroid and not centroid.is_empty\n",
    "                        else float(\"nan\"),\n",
    "                    }\n",
    "                )\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        if out_rows:\n",
    "            yield pd.DataFrame(out_rows)\n",
    "        else:\n",
    "            yield pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"source_file\",\n",
    "                    \"fototid\",\n",
    "                    \"geometry_wkt\",\n",
    "                    \"centroid_x\",\n",
    "                    \"centroid_y\",\n",
    "                ]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47cbe5e9-523a-464c-8da2-2ef1b89e789c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@F.pandas_udf(\"double\")\n",
    "def min_width_udf(wkt_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Returnerer en DataFrame med en kolonne med diameter for hver geometri.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for w in wkt_series:\n",
    "        geom = wkt.loads(w)\n",
    "        if isinstance(geom, MultiPolygon):\n",
    "            geom = max(geom.geoms, key=lambda g: g.area)\n",
    "        hull = geom.convex_hull\n",
    "        coords = list(hull.exterior.coords[:-1])\n",
    "        min_width = float(\"inf\")\n",
    "        for i in range(len(coords)):\n",
    "            a, b = coords[i], coords[(i + 1) % len(coords)]\n",
    "            dx, dy = b[0] - a[0], b[1] - a[1]\n",
    "            length = (dx**2 + dy**2) ** 0.5\n",
    "            if length == 0:\n",
    "                continue\n",
    "            perp_dx, perp_dy = -dy / length, dx / length\n",
    "            projections = [p[0] * perp_dx + p[1] * perp_dy for p in coords]\n",
    "            width = max(projections) - min(projections)\n",
    "            min_width = min(min_width, width)\n",
    "        results.append(min_width)\n",
    "\n",
    "    return pd.Series(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db6c44b-4f49-4b7f-9883-53cfce0c6eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Skriver data til deltatabellen og oppdaterer dersom row_hash allerede finnes.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5abc345f-0cf2-4d77-990a-f1d1b7efac90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Henter alle snuplasser fra bronsetabellen og skriver til deltatabellen.\n",
    "    \"\"\"\n",
    "    df_turning_spaces = spark.read.table(bronze_table).filter(F.col(\"turning_space\"))\n",
    "\n",
    "    # Henter fototid for hver snuplass\n",
    "    df_fototid = df_turning_spaces.withColumn(\n",
    "        \"nodeid\", F.expr(\"substring(source_file, 12, length(source_file)-15)\")\n",
    "    ).join(\n",
    "        spark.read.table(endepunkt_silver_table).select(\"nodeid\", \"fototid\"),\n",
    "        on=\"nodeid\",\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"source_file\", StringType(), False),\n",
    "            StructField(\"fototid\", StringType(), True),\n",
    "            StructField(\"geometry_wkt\", StringType(), False),\n",
    "            StructField(\"centroid_x\", DoubleType(), False),\n",
    "            StructField(\"centroid_y\", DoubleType(), False),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    df_grouped = df_fototid.mapInPandas(masks_grouped_udf, schema=schema)\n",
    "\n",
    "    # Finner minste diameter på tvers av polygonet for hver snuplass\n",
    "    sdf_diameter = df_grouped.withColumn(\n",
    "        \"diameter\", min_width_udf(F.col(\"geometry_wkt\"))\n",
    "    )\n",
    "    sdf_clean = sdf_diameter.drop(\"geometry_wkt\")\n",
    "\n",
    "    sdf = sdf_clean.withColumn(\"ingest_time\", F.current_timestamp()).withColumn(\n",
    "        \"row_hash\", F.sha2(F.concat_ws(\"||\", *sdf_clean.columns), 256)\n",
    "    )\n",
    "\n",
    "    write_delta_table(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbe83d20-7173-4873-8dd9-146265446fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_snuplasser_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
