{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25cce8d9-7437-47c4-9760-858e6527312c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import time\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import StructType, StructField, TimestampType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edef3876-8d9c-4c29-a940-52842d3e37a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "mask_path = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/predicted_masks\"\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "log_table = \"logs_predicted_masks\"\n",
    "table = \"predicted_bronze\"\n",
    "endepunkt_silver_table = \"endepunkt_silver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2564f5ec-c131-46c6-8292-d02ee3944254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {log_table} (\n",
    "  processed_time TIMESTAMP,\n",
    "  num_inserted INT,\n",
    "  num_updated INT,\n",
    "  num_deleted INT\n",
    "\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49678751-fb9a-4187-82ba-3f8de613f3c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "q = f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {table} (\n",
    "    white_pixels INT,\n",
    "    turning_space BOOLEAN,\n",
    "    bbox ARRAY<DOUBLE>,\n",
    "    source_file STRING,\n",
    "    row_hash STRING,\n",
    "    ingest_time TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\"\n",
    "spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e3bb5fb-1dfe-4cb7-8492-15c0c862e662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sett kontekst med katalog og skjema \n",
    "spark.sql(f'USE CATALOG {catalog_dev}')\n",
    "spark.sql(f'CREATE SCHEMA IF NOT EXISTS {schema_dev}')\n",
    "spark.sql(f'USE SCHEMA {schema_dev}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eafecc1-4635-4234-9e99-fac109c4f122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def log_predicted_mask(log_data: list):\n",
    "    \"\"\"\n",
    "    Writes the processed predicted mask to the log table.\n",
    "    \"\"\"\n",
    "    schema = StructType(\n",
    "        [\n",
    "            StructField(\"processed_time\", TimestampType(), True),\n",
    "            StructField(\"num_inserted\", IntegerType(), True),\n",
    "            StructField(\"num_updated\", IntegerType(), True),\n",
    "            StructField(\"num_deleted\", IntegerType(), True),\n",
    "        ]\n",
    "    )\n",
    "    spark.createDataFrame(log_data, schema=schema).write.format(\"delta\").mode(\n",
    "        \"append\"\n",
    "    ).saveAsTable(log_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb08fa4-246e-431b-acb7-d9b7e19f422b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_for_new_predicted_masks() -> list:\n",
    "    \"\"\"\n",
    "    Function that checks for new predicted mask. Returns a list of new predicted masks.\n",
    "    \"\"\"\n",
    "    all_masks = [\n",
    "        f.path.rstrip(\"/\").split(\"/\")[-1]\n",
    "        for f in dbutils.fs.ls(mask_path)\n",
    "        if f.path.endswith(\".png\")\n",
    "    ]\n",
    "    \n",
    "    processed_masks_df = spark.read.table(table).select(\"source_file\")\n",
    "    processed_masks = [row[\"source_file\"] for row in processed_masks_df.collect()]\n",
    "\n",
    "    return [mask for mask in all_masks if mask not in processed_masks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22689022-6aaa-4c84-9475-de7c12751107",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_buildings(bbox: str):\n",
    "    \"\"\"\n",
    "    Function that returns a GeoJSON of buildings within a bbox.\n",
    "    \"\"\"\n",
    "    bbox_str = \", \".join(map(str, bbox)) # Fjerner klammeparenteser\n",
    "    url = f\"https://openwms.statkart.no/skwms1/wms.fkb?VERSION=1.3.0&service=WMS&request=GetMap&Format=image/png&GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=bygning&BBox={bbox_str}&width=369&height=369\"\n",
    "    response = requests.get(url, timeout=10)\n",
    "    time.sleep(2)\n",
    "    img_gray = Image.open(BytesIO(response.content)).convert(\"L\")\n",
    "    arr = np.array(img_gray)\n",
    "    inv_arr = np.where(arr == 255, 0, 255).astype(np.uint8)\n",
    "    return inv_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79c1f804-23c0-4d6e-8a93-1e8d43839c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_bbox_from_table(nodeid: str) -> DataFrame:\n",
    "    df = spark.read.table(endepunkt_silver_table).filter(col(\"nodeid\") == nodeid).select(col(\"bbox\")).first().bbox\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937987c2-e091-49f6-95f2-db2a70336632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_sdf(predicted_masks: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read GeoJSON and write one merged MultiPolygon row to SDF with centroid.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    for mask in predicted_masks:\n",
    "        bbox = read_bbox_from_table(mask[11:-4])\n",
    "\n",
    "        img = Image.open(f\"{mask_path}/{mask}\").convert(\"L\")\n",
    "        arr = np.array(img)\n",
    "        inv_arr = get_buildings(bbox)\n",
    "        result_arr = np.clip(arr.astype(int) - inv_arr.astype(int), 0, 255).astype(np.uint8)\n",
    "        count_255 = int((result_arr == 255).sum())  \n",
    "\n",
    "        records.append(\n",
    "            {\n",
    "                \"white_pixels\": count_255,\n",
    "                \"turning_space\": bool(count_255 > 0),\n",
    "                \"bbox\": bbox,\n",
    "                \"source_file\": mask,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    sdf = spark.createDataFrame(records)\n",
    "\n",
    "    # Add metadata\n",
    "    sdf = sdf.withColumn(\n",
    "        \"row_hash\", sha2(concat_ws(\"||\", *sdf.columns), 256)\n",
    "    ).withColumn(\"ingest_time\", current_timestamp())\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44b044d-df56-4205-9c9b-25801c6adf0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    \"\"\"\n",
    "    Write delta table from spark dataframe.\n",
    "    \"\"\"\n",
    "    if not spark.catalog.tableExists(table):\n",
    "        sdf.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            source=sdf.alias(\"source\"), condition=\"target.row_hash = source.row_hash\"\n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.row_hash != source.row_hash\",\n",
    "            set={col: f\"source.{col}\" for col in sdf.columns},\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2caa379-fe3b-4b19-946a-c9bc552d7d10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_delta_table(predicted_masks: DataFrame):\n",
    "    \"\"\"\n",
    "    Updates the delta table and logs the predicted mask.\n",
    "    \"\"\"\n",
    "    table_exists = False\n",
    "    if spark.catalog.tableExists(table):\n",
    "        delta_tbl = DeltaTable.forName(spark, table)\n",
    "        version_before = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        table_exists = True\n",
    "\n",
    "    if predicted_masks:\n",
    "        sdf = write_to_sdf(predicted_masks)\n",
    "        write_delta_table(sdf)\n",
    "\n",
    "    if table_exists:\n",
    "        version_after = delta_tbl.history(1).select(\"version\").collect()[0][0]\n",
    "        if version_after > version_before:\n",
    "            metrics = delta_tbl.history(1).select(\"operationMetrics\").collect()[0][0]\n",
    "            updated = int(metrics.get(\"numTargetRowsUpdated\", 0))\n",
    "            inserted = int(metrics.get(\"numTargetRowsInserted\", 0))\n",
    "            deleted = int(metrics.get(\"numTargetRowsDeleted\", 0))\n",
    "            print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "        else:\n",
    "            inserted, updated, deleted = 0, 0, 0\n",
    "            print(\"No new Delta version found after merge.\")\n",
    "    else:\n",
    "        inserted, updated, deleted = sdf.count(), 0, 0\n",
    "        print(f\"Updated: {updated}, Inserted: {inserted}, Deleted: {deleted}\")\n",
    "\n",
    "    log_predicted_mask(log_data=[(datetime.now(), inserted, updated, deleted)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b740c69b-9329-40a2-8d21-7dacf64bfa19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Reads predicted masks and writes them to delta table.\n",
    "    \"\"\"\n",
    "    predicted_masks = check_for_new_predicted_masks()\n",
    "    write_to_delta_table(predicted_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "203f4fb1-4776-4efb-aa30-89e5b9aa7714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "predicted_bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
