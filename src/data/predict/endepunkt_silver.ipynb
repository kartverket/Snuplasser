{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46b62b6e-f04e-441f-9e22-848fb394c19f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f877bc39-e6ba-43e0-9eb6-27aca092332b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DataType, DateType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4319e3e5-9801-410e-b8a7-44c651d396d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "bronze_table = \"endepunkt_bronze\"\n",
    "silver_table = \"endepunkt_silver\"\n",
    "buffer= 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aee570a-0379-435d-b303-db1d6c92da0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dom_url(bbox):\n",
    "    bbox_str = \",\".join(map(str, bbox))\n",
    "    width, height = 512, 512\n",
    "    resolution =0.2\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.hoyde-dom-nhm-25833?request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=NHM_DOM_25833:skyggerelieff&\"\n",
    "        f\"BBOX={bbox_str}&width={width}&height={height}&RESOLUTION={resolution}\"\n",
    "    )\n",
    "\n",
    "generate_dom_url_udf = udf(generate_dom_url, StringType())\n",
    "\n",
    "def dom_file_exists(nodeid: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/storreendepunkt_dom/dom_{nodeid}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "dom_file_status_udf = udf(dom_file_exists, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21faddbd-b620-48ec-9e68-30b8514d36e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_url(bbox):\n",
    "    try:\n",
    "        bbox_str = \",\".join(map(str, bbox))\n",
    "        width, height = 512, 512\n",
    "        resolution = 0.2\n",
    "        return (\n",
    "            f\"https://wms.geonorge.no/skwms1/wms.nib?VERSION=1.3.0\"\n",
    "            f\"&service=WMS&request=GetMap&Format=image/png&\"\n",
    "            f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=ortofoto&\"\n",
    "            f\"BBox={bbox_str}&width={width}&height={height}&RESOLUTION={resolution}&TICKET=\"\n",
    "        ) \n",
    "    except Exception as e:\n",
    "        return \"INVALID\"\n",
    "    \n",
    "generate_image_url_udf = udf(generate_image_url, StringType())\n",
    "\n",
    "def image_file_exists(nodeid: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/storreendepunkt_images/image_{nodeid}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "image_file_status_udf = udf(image_file_exists, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863ddf55-1a73-4086-8cfd-a54715d170ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BRUKERID = os.getenv(\"GEONORGE_BRUKERID\")\n",
    "PASSORD  = os.getenv(\"GEONORGE_PASSORD\")\n",
    "\n",
    "def get_token():\n",
    "    url = (\n",
    "        f\"https://baat.geonorge.no/skbaatts/req?brukerid={BRUKERID}\"\n",
    "        f\"&passord={PASSORD}&tjenesteid=wms.nib&retformat=s\"\n",
    "    )\n",
    "    raw_token = requests.get(url).text.strip(\"`\")\n",
    "    return raw_token\n",
    "\n",
    "token = get_token()\n",
    "token_start_time = time.time()\n",
    "token_lifetime = 55 * 60  \n",
    "\n",
    "def refresh_token_if_needed():\n",
    "    global token, token_start_time\n",
    "    if time.time() - token_start_time > token_lifetime:\n",
    "        print(\"ðŸ”„ Fornyer token...\")\n",
    "        token = get_token()\n",
    "        token_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5465ced6-a397-4f0d-b426-4fbc1dff84f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_fotodato(bbox: str, token: str):\n",
    "    url = f\"https://wms.geonorge.no/skwms1/wms.nib?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&CRS=EPSG:25833&BBOX={bbox}&WIDTH=512&HEIGHT=512&LAYERS=ortofoto&QUERY_LAYERS=ortofoto&INFO_FORMAT=text/html&I=256&J=256&TICKET={token}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\")\n",
    "    field_value = None\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) >= 2 and cells[0].text.strip() == \"Fotodato\":\n",
    "                field_value = cells[1].text.strip()\n",
    "                field_value = datetime.strptime(field_value, \"%d.%m.%Y\").date()\n",
    "                return field_value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99f82343-c8c7-43fd-82e5-f8b2f49afaac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ortofoto_date(df, token: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Add the ortofoto date to the dataframe based on bbox.\n",
    "    Returns a new DataFrame with the 'fototid' column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only relevant columns for processing\n",
    "    sample_rows = df.select(\"row_hash\", \"bbox_str\").collect()\n",
    "\n",
    "    # Fetch fotodato values from WMS\n",
    "    bbox_date_pairs = [\n",
    "        (row[\"row_hash\"], get_fotodato(row[\"bbox_str\"].replace('_', ','), token))\n",
    "        for row in sample_rows\n",
    "    ]\n",
    "\n",
    "    # Schema for intermediate DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"row_hash\", StringType(), True),\n",
    "        StructField(\"fototid\", DateType(), True)  # Use StringType if needed\n",
    "    ])\n",
    "\n",
    "    # Create small lookup DataFrame\n",
    "    bbox_date_df = spark.createDataFrame(bbox_date_pairs, schema)\n",
    "\n",
    "    # Join back on row_hash\n",
    "    df_with_date = df.join(bbox_date_df, on=\"row_hash\", how=\"left\")\n",
    "\n",
    "    return df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d91f70e-643a-48a0-a189-0777658ca118",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER\"\n",
    "\n",
    "def add_silver_columns(df: DataFrame, buffer: Optional[int], kommune_id: str = \"\") -> DataFrame:\n",
    "    buffer = buffer or 64\n",
    "    df = df.withColumn(\"bbox\", expr(f\"array(x - {buffer}, y - {buffer}, x + {buffer}, y + {buffer})\"))\n",
    "    df = df.withColumn(\"bbox_str\", concat_ws(\",\", col(\"bbox\"))) \n",
    "    df = df.withColumn(\"image_wms\", generate_image_url_udf(col(\"bbox\"))) \\\n",
    "           .withColumn(\"dom_wms\", generate_dom_url_udf(col(\"bbox\"))) \\\n",
    "           .withColumn(\"image_status\", image_file_status_udf(col(\"nodeid\"))) \\\n",
    "           .withColumn(\"dom_status\", dom_file_status_udf(col(\"nodeid\"))) \\\n",
    "           .withColumn(\"lastet_tid\", current_timestamp()) \\\n",
    "           .withColumn(\"kommune_id\", lit(kommune_id)) \\\n",
    "           .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "    df = df.withColumn(\"image_path\", concat(lit(BASE_PATH), lit(\"/storreendepunkt_images/image_\"), col(\"nodeid\"), lit(\".png\")))\n",
    "    df = df.withColumn(\"dom_path\", concat(lit(BASE_PATH), lit(\"/storreendepunkt_dom/dom_\"), col(\"nodeid\"), lit(\".png\")))\n",
    "    df = add_ortofoto_date(df, token)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088d45e0-a644-48dc-97ec-91a90172a047",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame, mode: str = \"merge\") -> None:\n",
    "    if mode == \"overwrite\":\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        from delta.tables import DeltaTable\n",
    "\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            sdf.alias(\"source\"),\n",
    "            condition=\"target.nodeid = source.nodeid\" \n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.hentet_tid < source.hentet_tid OR target.image_path IS NULL\", \n",
    "            set={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f435e334-a27a-4a8a-b89c-6004e24c8660",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_for_kommune(kommune_id: str) -> None:\n",
    "    kommune_id = str(kommune_id) \n",
    "    bronze_df = spark.read.table(bronze_table)\n",
    "    bronze_df = bronze_df.filter(col(\"kommune_id\") == lit(kommune_id))\n",
    "\n",
    "    silver_df = add_silver_columns(bronze_df, buffer=buffer, kommune_id=kommune_id)\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        write_delta_table(silver_df, mode=\"overwrite\")\n",
    "        return\n",
    "    \n",
    "    expected_schema = spark.table(silver_table).schema\n",
    "\n",
    "    silver_df = silver_df.select([\n",
    "        lit(\"\").cast(\"string\").alias(c.name) if c.dataType.typeName() == \"void\"\n",
    "        else col(c.name).cast(c.dataType)\n",
    "        for c in expected_schema\n",
    "    ])\n",
    "\n",
    "    w = Window.partitionBy(\"nodeid\").orderBy(col(\"hentet_tid\").desc())\n",
    "    silver_df = silver_df.withColumn(\"row_number\", row_number().over(w)) \\\n",
    "                          .filter(col(\"row_number\") == lit(1)) \\\n",
    "                          .drop(\"row_number\")\n",
    "\n",
    "    write_delta_table(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7afe1339-3252-4fa0-94c7-e13866ebcaee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kommune_id_rows = [\n",
    "    row.asDict() for row in spark.read.table(bronze_table).select(\"kommune_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "for row in kommune_id_rows:\n",
    "    print(f\"Row: {row}, type: {type(row)}, kommune_id: {row['kommune_id']}, type: {type(row['kommune_id'])}\")\n",
    "\n",
    "    kommune_id = row[\"kommune_id\"]\n",
    "    if kommune_id is not None:\n",
    "        kommune_id = str(kommune_id)\n",
    "        process_silver_for_kommune(kommune_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd6fae0-b9a9-497e-8339-3feb9c610be3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(silver_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb435fa3-2f80-4ebc-930f-d77100917b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# df_retable = spark.read.table(silver_table)\n",
    "# df_overview = df_retable.drop(\n",
    "#     \"Shape_Length\",\n",
    "#     \"Shape_Area\",\n",
    "#     \"Vegtyper\",\n",
    "#     \"Snuplasstype\",\n",
    "#     \"Parkering\",\n",
    "#     \"geometry\",\n",
    "#     \"ingest_time\",\n",
    "#     \"source_file\",\n",
    "#     \"source_layer\",\n",
    "#     \"bbox\",\n",
    "#     \"Polygons\",\n",
    "#     \"adjusted_struct\",\n",
    "#     \"Adjusted_bbox\",\n",
    "#     \"bbox_str\",\n",
    "#     \"image_wms\",\n",
    "#     \"dom_wms\",\n",
    "#     \"lastet_tid\",\n",
    "#     \"fototid\"\n",
    "# )\n",
    "\n",
    "# df_overview.write \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .option(\"mergeSchema\", \"true\") \\\n",
    "#     .saveAsTable(\"endepunkt_status_overview\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "endepunkt_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
