{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f3216c2-0058-471a-9750-ad2fe9e1a386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40d2faf-3d30-44ac-846b-d5bbf7bb6691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import requests\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae7089e-f8a0-44d1-8ef3-69724537f6b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")\n",
    "\n",
    "bronze_table = \"endepunkt_bronze\"\n",
    "silver_table = \"endepunkt_silver\"\n",
    "buffer = 128  # Gir bildeomrÃ¥det (meter) 256*256 med pikselareal 0.25kvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78621ec-fa17-41d2-bf14-acd06566bca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dom_url(bbox):\n",
    "    bbox_str = \",\".join(map(str, bbox))\n",
    "    width, height = 512, 512\n",
    "    resolution = 0.5  # Styrer pikselareal. 0.5 gir 0.25kvm pikselareal sammen med buffer = 128\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.hoyde-dom-nhm-25833?request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=NHM_DOM_25833:skyggerelieff&\"\n",
    "        f\"BBOX={bbox_str}&width={width}&height={height}&RESOLUTION={resolution}\"\n",
    "    )\n",
    "\n",
    "generate_dom_url_udf = udf(generate_dom_url, StringType())\n",
    "\n",
    "def dom_file_exists(nodeid: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/endepunkt_dom/dom_{nodeid}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "dom_file_status_udf = udf(dom_file_exists, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95f430a3-07b4-4b96-91e1-365918965714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_image_url(bbox):\n",
    "    try:\n",
    "        bbox_str = \",\".join(map(str, bbox))\n",
    "        width, height = 512, 512\n",
    "        resolution = 0.5\n",
    "        return (\n",
    "            f\"https://wms.geonorge.no/skwms1/wms.nib?VERSION=1.3.0\"\n",
    "            f\"&service=WMS&request=GetMap&Format=image/png&\"\n",
    "            f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=ortofoto&\"\n",
    "            f\"BBox={bbox_str}&width={width}&height={height}&RESOLUTION={resolution}&TICKET=\"\n",
    "        )  # token legges til etter TICKET nÃ¥r UDF kjÃ¸rer\n",
    "    except Exception as e:\n",
    "        return \"INVALID\"\n",
    "    \n",
    "generate_image_url_udf = udf(generate_image_url, StringType())\n",
    "\n",
    "def image_file_exists(nodeid: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/endepunkt_images/image_{nodeid}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "image_file_status_udf = udf(image_file_exists, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05852536-5eda-4e29-8f66-29478b3167d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "BRUKERID = os.getenv(\"GEONORGE_BRUKERID\")\n",
    "PASSORD  = os.getenv(\"GEONORGE_PASSORD\")\n",
    "\n",
    "def get_token():\n",
    "    url = (\n",
    "        f\"https://baat.geonorge.no/skbaatts/req?brukerid={BRUKERID}\"\n",
    "        f\"&passord={PASSORD}&tjenesteid=wms.nib&retformat=s\"\n",
    "    )\n",
    "    raw_token = requests.get(url).text.strip(\"`\")\n",
    "    return raw_token\n",
    "\n",
    "token = get_token()\n",
    "token_start_time = time.time()\n",
    "token_lifetime = 55 * 60  # sekunder\n",
    "\n",
    "def refresh_token_if_needed():\n",
    "    global token, token_start_time\n",
    "    if time.time() - token_start_time > token_lifetime:\n",
    "        print(\"ðŸ”„ Fornyer token...\")\n",
    "        token = get_token()\n",
    "        token_start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdda1d30-6abc-47c4-9e7f-63d394fadcad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_fotodato(bbox: str, token: str):\n",
    "    url = f\"https://wms.geonorge.no/skwms1/wms.nib?SERVICE=WMS&VERSION=1.3.0&REQUEST=GetFeatureInfo&CRS=EPSG:25833&BBOX={bbox}&WIDTH=512&HEIGHT=512&LAYERS=ortofoto&QUERY_LAYERS=ortofoto&INFO_FORMAT=text/html&I=256&J=256&TICKET={token}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    table = soup.find(\"table\")\n",
    "    field_value = None\n",
    "    \n",
    "    if table:\n",
    "        rows = table.find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            if len(cells) >= 2 and cells[0].text.strip() == \"Fotodato\":\n",
    "                field_value = cells[1].text.strip()\n",
    "                field_value = datetime.strptime(field_value, \"%d.%m.%Y\").date()\n",
    "                return field_value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd7f2a92-6302-4644-8a95-a2158c36a087",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_ortofoto_date(df, token: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Add the ortofoto date to the dataframe based on bbox.\n",
    "    Returns a new DataFrame with the 'fototid' column added.\n",
    "    \"\"\"\n",
    "\n",
    "    # Select only relevant columns for processing\n",
    "    sample_rows = df.select(\"row_hash\", \"bbox_str\").collect()\n",
    "\n",
    "    # Fetch fotodato values from WMS\n",
    "    bbox_date_pairs = [\n",
    "        (row[\"row_hash\"], get_fotodato(row[\"bbox_str\"].replace('_', ','), token))\n",
    "        for row in sample_rows\n",
    "    ]\n",
    "\n",
    "    # Schema for intermediate DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"row_hash\", StringType(), True),\n",
    "        StructField(\"fototid\", DateType(), True)  # Use StringType if needed\n",
    "    ])\n",
    "\n",
    "    # Create small lookup DataFrame\n",
    "    bbox_date_df = spark.createDataFrame(bbox_date_pairs, schema)\n",
    "\n",
    "    # Join back on row_hash\n",
    "    df_with_date = df.join(bbox_date_df, on=\"row_hash\", how=\"left\")\n",
    "\n",
    "    return df_with_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ee2a6a-1f78-4940-bf5c-a85829ef4794",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER\"\n",
    "\n",
    "def add_silver_columns(df: DataFrame, buffer: Optional[int], kommune_id: str = \"\") -> DataFrame:\n",
    "    buffer = buffer or 128\n",
    "    df = df.withColumn(\"bbox\", expr(f\"array(x - {buffer}, y - {buffer}, x + {buffer}, y + {buffer})\"))\n",
    "    df = df.withColumn(\"image_wms\", generate_image_url_udf(col(\"bbox\"))) \\\n",
    "           .withColumn(\"dom_wms\", generate_dom_url_udf(col(\"bbox\"))) \\\n",
    "           .withColumn(\"image_status\", image_file_status_udf(col(\"nodeid\"))) \\\n",
    "           .withColumn(\"dom_status\", dom_file_status_udf(col(\"nodeid\"))) \\\n",
    "           .withColumn(\"lastet_tid\", current_timestamp()) \\\n",
    "           .withColumn(\"kommune_id\", lit(kommune_id)) \\\n",
    "           .withColumn(\"row_hash\", sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "    df = df.withColumn(\"image_path\", concat(lit(BASE_PATH), lit(\"/endepunkt_images/image_\"), col(\"nodeid\"), lit(\".png\")))\n",
    "    df = df.withColumn(\"dom_path\", concat(lit(BASE_PATH), lit(\"/endepunkt_dom/dom_\"), col(\"nodeid\"), lit(\".png\")))\n",
    "    df = add_ortofoto_date(df, token)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60934ed-4c02-4729-8606-173646773b6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame, mode: str = \"merge\") -> None:\n",
    "    if mode == \"overwrite\":\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        from delta.tables import DeltaTable\n",
    "\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "\n",
    "        delta_tbl.alias(\"target\").merge(\n",
    "            sdf.alias(\"source\"),\n",
    "            condition=\"target.nodeid = source.nodeid\" \n",
    "        ).whenMatchedUpdate(\n",
    "            condition=\"target.hentet_tid < source.hentet_tid OR target.image_path IS NULL\", \n",
    "            set={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).whenNotMatchedInsert(\n",
    "            values={col: f\"source.{col}\" for col in sdf.columns}\n",
    "        ).execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13479b8-10b3-4813-926f-c7e0382050de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_for_kommune(kommune_id: str) -> None:\n",
    "    kommune_id = str(kommune_id)  # sikkerhet\n",
    "    bronze_df = spark.read.table(bronze_table)\n",
    "    bronze_df = bronze_df.filter(col(\"kommune_id\") == lit(kommune_id))\n",
    "\n",
    "    silver_df = add_silver_columns(bronze_df, buffer=buffer, kommune_id=kommune_id)\n",
    "\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        write_delta_table(silver_df, mode=\"overwrite\")\n",
    "        return\n",
    "    \n",
    "    expected_schema = spark.table(silver_table).schema\n",
    "\n",
    "    silver_df = silver_df.select([\n",
    "        lit(\"\").cast(\"string\").alias(c.name) if c.dataType.typeName() == \"void\"\n",
    "        else col(c.name).cast(c.dataType)\n",
    "        for c in expected_schema\n",
    "    ])\n",
    "\n",
    "    w = Window.partitionBy(\"nodeid\").orderBy(col(\"hentet_tid\").desc())\n",
    "    silver_df = silver_df.withColumn(\"row_number\", row_number().over(w)) \\\n",
    "                          .filter(col(\"row_number\") == lit(1)) \\\n",
    "                          .drop(\"row_number\")\n",
    "\n",
    "    write_delta_table(silver_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "855fc0c6-e6c2-4965-b1e3-03cad9032473",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "kommune_id_rows = [\n",
    "    row.asDict() for row in spark.read.table(bronze_table).select(\"kommune_id\").distinct().collect()\n",
    "]\n",
    "\n",
    "for row in kommune_id_rows:\n",
    "    print(f\"Row: {row}, type: {type(row)}, kommune_id: {row['kommune_id']}, type: {type(row['kommune_id'])}\")\n",
    "\n",
    "    kommune_id = row[\"kommune_id\"]\n",
    "    if kommune_id is not None:\n",
    "        kommune_id = str(kommune_id)\n",
    "        process_silver_for_kommune(kommune_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08220333-f645-4764-af1b-a5b3fb708a5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f579b2a-a88d-446a-8e96-a6ec536fabf1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"image_path\":1500,\"dom_path\":1358},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"image_wms\":{\"format\":{\"preset\":\"string-preset-url\"}},\"dom_wms\":{\"format\":{\"preset\":\"string-preset-url\"}},\"image_path\":{\"format\":{\"preset\":\"string-preset-url\"}},\"dom_path\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1754393124883}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.table(silver_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12ce06e6-3f31-4f8b-83f9-b292af32f121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_retable = spark.read.table(silver_table)\n",
    "df_overview = df_retable.drop(\"x\", \"y\", \"wkt\", \"kommune_id\", \"hentet_tid\", \"row_hash\", \"bbox\", \"image_wms\", \"dom_wms\",  \"lastet_tid\", \"fototid\")\n",
    "\n",
    "df_overview.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"endepunkt_status_overview\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "endepunkt_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
