{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed1b74a3-f832-42dd-ac71-fdfc4ed71bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ce3e967-c0e0-4bf1-82ed-911d7b28d1c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import min as spark_min, max as spark_max, when \n",
    "from pyspark.sql.types import ArrayType, DoubleType, StringType, StructType, StructField, IntegerType, LongType, FloatType\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from sedona.spark import *\n",
    "\n",
    "import random\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "from shapely.errors import WKTReadingError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e43ca0-cc6e-43d2-877e-f41d8f77001b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "catalog_dev = \"`land_topografisk-gdb_dev`\"\n",
    "schema_dev = \"ai2025\"\n",
    "\n",
    "bronze_table = f\"{catalog_dev}.{schema_dev}.utensnuplass_bronze\"\n",
    "silver_table = f\"{catalog_dev}.{schema_dev}.utensnuplass_silver\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog_dev}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {schema_dev}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a4786fe-f2cb-4f33-ba78-ceabd1d61c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_geometry_from_column()-> DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the geometries from the bronze table and returns a dataframe with the geometries as a column.\n",
    "    \"\"\"\n",
    "    df_bronze= spark.read.table(bronze_table).withColumn(\"geometry\", F.expr(\"ST_GeomFromWKT(bbox)\"))\n",
    "    return df_bronze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5d557b5-79d4-490d-804b-62101360a7eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_envolope_column(df:DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Adds a column with the envelope of the geometries.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\"envelope\", F.expr(\"ST_Boundary(geometry)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d3a263-fcfa-4109-adcc-ca08342531ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def random_adjusted_bbox_centered(\n",
    "    envelope: list,\n",
    "    min_size: int = 256,\n",
    "    max_size: int = 256,\n",
    "    margin: int = 30,\n",
    "    max_offset: float = 80,  # margin - f√• meter\n",
    "    max_attempts: int = 10\n",
    ") -> list:\n",
    "    \n",
    "    import random\n",
    "\n",
    "    xmin, ymin, xmax, ymax = envelope\n",
    "    poly_width = xmax - xmin\n",
    "    poly_height = ymax - ymin\n",
    "\n",
    "    # Beregn √∏nsket BBOX-st√∏rrelse\n",
    "    bbox_size = max(poly_width, poly_height) + margin * 2\n",
    "    bbox_size = min(max(bbox_size, min_size), max_size)\n",
    "    half_size = bbox_size / 2\n",
    "\n",
    "    # Polygonets sentrum\n",
    "    center_x_orig = (xmin + xmax) / 2\n",
    "    center_y_orig = (ymin + ymax) / 2\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        dx = random.uniform(-max_offset, max_offset)\n",
    "        dy = random.uniform(-max_offset, max_offset)\n",
    "\n",
    "        center_x = center_x_orig + dx\n",
    "        center_y = center_y_orig + dy\n",
    "\n",
    "        # Lag BBOX\n",
    "        adjusted_xmin = center_x - half_size\n",
    "        adjusted_xmax = center_x + half_size\n",
    "        adjusted_ymin = center_y - half_size\n",
    "        adjusted_ymax = center_y + half_size\n",
    "\n",
    "        # Sjekk at hele polygonet er innenfor den justerte BBOX-en\n",
    "        if (adjusted_xmin <= xmin and adjusted_ymin <= ymin and\n",
    "            adjusted_xmax >= xmax and adjusted_ymax >= ymax):\n",
    "            bbox = [adjusted_xmin, adjusted_ymin, adjusted_xmax, adjusted_ymax]\n",
    "            bbox_str = \"_\".join(f\"{v:.6f}\" for v in bbox)\n",
    "            return {\"bbox\" : bbox, \"bbox_str\" : bbox_str}\n",
    "\n",
    "    raise ValueError(\"Fant ikke gyldig adjusted_bbox etter flere fors√∏k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc5efd03-3fce-4eb4-8440-974cf5707245",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def make_bbox(df: DataFrame, buffer: float = 20.0) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Generates a bounding box for each row based on the 'envelope' geometry,\n",
    "    expands it slightly with a buffer, and returns random adjusted boxes.\n",
    "\n",
    "    Args:\n",
    "        df: Spark DataFrame with a 'geometry' column (as ST_GeomFromWKT).\n",
    "        buffer: Extra margin (in meters) added around the bounding box.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with added columns: bbox, bbox_str, Adjusted_bbox, Polygons\n",
    "    \"\"\"\n",
    "\n",
    "    # Create bounding box array from envelope\n",
    "    df = df.withColumn(\n",
    "        \"bbox\",\n",
    "        F.expr(f\"\"\"\n",
    "        array(\n",
    "            ST_X(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_Y(ST_Centroid(envelope)) - (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_X(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer}),\n",
    "            ST_Y(ST_Centroid(envelope)) + (GREATEST(ST_XMax(envelope) - ST_XMin(envelope), ST_YMax(envelope) - ST_YMin(envelope)) / 2 + {buffer})\n",
    "        )\n",
    "        \"\"\")\n",
    "    )\n",
    "\n",
    "    # Turn bbox array into ST_Polygon\n",
    "    df = df.withColumn(\n",
    "        \"Polygons\",\n",
    "        F.expr(\"ST_MakeEnvelope(bbox[0], bbox[1], bbox[2], bbox[3])\")\n",
    "    )\n",
    "\n",
    "    # Apply adjusted BBOX logic using the UDF\n",
    "    df = df.withColumn(\n",
    "        \"adjusted_struct\",\n",
    "        adjusted_bbox_udf(F.col(\"bbox\"))\n",
    "    ).withColumn(\n",
    "        \"Adjusted_bbox\", F.col(\"adjusted_struct.bbox\")\n",
    "    ).withColumn(\n",
    "        \"bbox_str\", F.col(\"adjusted_struct.bbox_str\")\n",
    "    ).drop(\"envelope\")  # Drop envelope if no longer needed\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed73be2-d7d4-4488-97a2-e01841e5e55a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_blank_mask(id:str, save_dir: str= \"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSlabel\"):\n",
    "\n",
    "    \"\"\"\n",
    "    Generates a blank mask image for a given ID and saves it to the specified directory.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path= os.path.join(save_dir, f\"{id}.png\")\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    mask = Image.new(\"L\", (256, 256), color=0)\n",
    "    mask.save(file_path)\n",
    "\n",
    "def generate_blank_masks_for_pending(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Creates blank masks only for rows where mask_status is 'PENDING'.\n",
    "    \"\"\"\n",
    "    pending = df[df[\"mask_status\"] == \"PENDING\"]\n",
    "    print(f\"{len(pending)} mask(s) will be generated.\")\n",
    "\n",
    "    for row in pending.itertuples():\n",
    "        generate_blank_mask(row.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060571aa-5e5a-41a4-b814-ab3bb28bff3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_dom_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.hoyde-dom-nhm-25833?request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=NHM_DOM_25833:skyggerelieff&\"\n",
    "        f\"BBOX={bbox_str}&width={width}&height={height}\"\n",
    "    )\n",
    "\n",
    "def generate_image_url(bbox_str):\n",
    "    width, height = [512, 512]\n",
    "    return (\n",
    "        f\"https://wms.geonorge.no/skwms1/wms.nib?VERSION=1.3.0\"\n",
    "        f\"&service=WMS&request=GetMap&Format=image/png&\"\n",
    "        f\"GetFeatureInfo=text/plain&CRS=EPSG:25833&Layers=ortofoto&\"\n",
    "        f\"BBox={bbox_str}&width={width}&height={height}&TICKET=\"\n",
    "    )\n",
    "\n",
    "def dom_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSdom/dom_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def image_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSimage/image_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\"\n",
    "\n",
    "def mask_file_exists(id: str) -> str:\n",
    "    path = f\"/Volumes/land_topografisk-gdb_dev/external_dev/static_data/DL_SNUPLASSER/utenSlabel/mask_{id}.png\"\n",
    "    return \"DOWNLOADED\" if os.path.exists(path) else \"PENDING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6914a658-0579-401f-9ecf-6bcbc2c0ba30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_delta_table(sdf: DataFrame):\n",
    "    if not spark.catalog.tableExists(silver_table):\n",
    "        sdf.write.format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .saveAsTable(silver_table)\n",
    "    else:\n",
    "        delta_tbl = DeltaTable.forName(spark, silver_table)\n",
    "        delta_tbl.alias(\"target\") \\\n",
    "            .merge(\n",
    "                source=sdf.alias(\"source\"),\n",
    "                condition=\"target.row_hash = source.row_hash\"\n",
    "            ) \\\n",
    "            .whenMatchedUpdateAll() \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "600bb3b3-1794-4a0d-9e90-8b4284c53896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def safe_load_wkt(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            return wkt.loads(val)\n",
    "        except WKTReadingError:\n",
    "            print(\"‚ùå Hatalƒ± WKT:\", val)\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def to_geopandas(df: DataFrame, column_name: str):\n",
    "    pdf = df.toPandas()\n",
    "    print(\"üîç DataFrame Pandas'a d√∂n√º≈üt√ºr√ºld√º.\")\n",
    "    print(\"Kolonlar:\", pdf.columns)\n",
    "    print(\"ƒ∞lk 3 satƒ±r (WKT):\", pdf[column_name].head(3))\n",
    "\n",
    "    pdf[\"geometry\"] = pdf[column_name].apply(safe_load_wkt)\n",
    "    return gpd.GeoDataFrame(pdf, geometry=\"geometry\", crs=\"EPSG:25833\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36eddef6-8564-41a0-83c7-abb3b9783e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adjusted_bbox_schema = StructType([\n",
    "    StructField(\"bbox\", ArrayType(DoubleType())),\n",
    "    StructField(\"bbox_str\", StringType())\n",
    "])\n",
    "adjusted_bbox_udf = F.udf(lambda envelope: random_adjusted_bbox_centered(envelope), adjusted_bbox_schema)\n",
    "generate_dom_url_udf = F.udf(generate_dom_url, StringType())\n",
    "generate_image_url_udf = F.udf(generate_image_url, StringType())\n",
    "dom_file_exists_udf = F.udf(dom_file_exists, StringType())\n",
    "image_file_exists_udf = F.udf(image_file_exists, StringType())\n",
    "mask_file_exists_udf = F.udf(mask_file_exists, StringType())\n",
    "\n",
    "df= load_geometry_from_column()\n",
    "df = add_envolope_column(df)\n",
    "df = make_bbox(df)\n",
    "df = df.withColumn(\"image_path\", generate_image_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"dom_path\", generate_dom_url_udf(\"Adjusted_bbox\")) \\\n",
    "       .withColumn(\"image_status\", image_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"dom_status\", dom_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"mask_status\", mask_file_exists_udf(\"row_hash\")) \\\n",
    "       .withColumn(\"lastet_tid\", F.current_timestamp())\n",
    "\n",
    "gdf= to_geopandas(df, \"bbox\")\n",
    "generate_blank_masks_for_pending(gdf) \n",
    "write_delta_table(df)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790a419a-55be-4bbe-96fc-63f36d6fad4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "705f4559-e0d9-42f3-93a8-0bb5b0ec1e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_retable = spark.read.table(silver_table)\n",
    "df_overview = df_retable.drop(\n",
    "    \"Shape_Length\",\n",
    "    \"Shape_Area\",\n",
    "    \"bbox\",\n",
    "    \"id\",\n",
    "    \"ingest_time\",\n",
    "    \"source_file\",\n",
    "    \"geometry\",\n",
    "    \"Polygons\",\n",
    "    \"adjusted_struct\",\n",
    "    \"Adjusted_bbox\",\n",
    "    \"bbox_str\",\n",
    "    \"image_wms\",\n",
    "    \"dom_wms\",\n",
    "    \"lastet_tid\"\n",
    ")\n",
    " \n",
    "df_overview.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(\"utenSnuplass_status_overview\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "utenSnuplass_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
